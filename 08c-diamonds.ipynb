{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"08c-diamonds.ipynb","provenance":[{"file_id":"1EvJv08XiHE58ORxqJbQG3dhr2LztE3Ez","timestamp":1626370542575}],"collapsed_sections":[],"authorship_tag":"ABX9TyNeQfR+7a09+A7aRHLV1iyJ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"sfmkoA8XXKPH"},"source":["<table align=\"center\">\n","   <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ds5110/summer-2021/blob/master/08c-diamonds.ipynb\">\n","<img src=\"https://github.com/ds5110/summer-2021/raw/master/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"iSxrt0jKnBkD"},"source":["# 8c -- diamonds\n","\n","Linear regression with the diamonds dataset (continued)\n","\n","In module 6, we covered:\n","\n","* Visualizing regression models with Seaborn\n","* Linear regression with linear model\n","* Nonlinearity (linear regression with polynomials)\n","* Scaling -- price vs carats\n","\n","In this module, we'll use the same dataset to cover:\n","\n","* Using residuals to assess statistical assumptions\n","* Investigate distributions with histograms to distinguish signal vs noise\n","* Q-Q plots -- visual assessment of data/model distributions\n","* Box plots (justifying multiple input/feature model)\n","* Adding categorical features (forward selection)"]},{"cell_type":"markdown","metadata":{"id":"9VLpG1BdI2iL"},"source":["## diamonds dataset\n","\n","* [diamonds dataset](https://ggplot2.tidyverse.org/reference/diamonds.html) -- tidyverse.org\n","\n"]},{"cell_type":"code","metadata":{"id":"8GOzN7u0I_3i"},"source":["import pandas as pd\n","\n","url = \"https://github.com/tidyverse/ggplot2/raw/master/data-raw/diamonds.csv\"\n","diamonds = pd.read_csv(url)\n","\n","diamonds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FvWAnn9d15iS"},"source":["import matplotlib.pyplot as plt\n","\n","# Use opacity (alpha channel) to aid visualization with large amounts of data\n","plt.scatter(diamonds['carat'], diamonds['price'], alpha=.1);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VHe22Mv4GhoN"},"source":["# Linear regression with scikit-learn\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import explained_variance_score\n","\n","# Use the entire dataset\n","df = diamonds\n","\n","# Fit the model to the data\n","model = LinearRegression()\n","model.fit(df[['carat']], df['price'])\n","\n","# Evaluate the performance\n","yhat = model.predict(df[['carat']])\n","print('Explained variance: {:.2f}%'.format(100 * explained_variance_score(df['price'], yhat)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e8WQnqdVMWcu"},"source":["# Plot the model prediction (no noise) as a smooth curve\n","X_model = [[0], [3]] # dependent variable, no noise\n","yhat_model = model.predict(X_model) # compute the model prediction\n","\n","plt.plot(X_model, yhat_model, color='k')\n","\n","# Plot the data (scatterplot)\n","plt.scatter(df['carat'], df['price'], alpha=.1)\n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nFnf-CfUSCvN"},"source":["# Evaluate model performance\n","\n","Investigate two methods of creating a train/test split.\n"]},{"cell_type":"code","metadata":{"id":"YJHdx1WoTtGj"},"source":["# Train-test split with scikit-learn\n","from sklearn.model_selection import train_test_split\n","\n","# Train-test split -- split the dataset\n","def my_train_test_split(X, y, test_size=.3):\n","    n_training_samples = int((1.0 - test_size) * X.shape[0])\n","\n","    X_train = X[:n_training_samples,:]\n","    y_train = y[:n_training_samples]\n","\n","    X_test = X[n_training_samples:,:]\n","    y_test = y[n_training_samples:]\n","\n","    return X_train, X_test, y_train, y_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MilAIXuz7uwF"},"source":["# Train/test split\n","\n","Choose one of two methods:\n","\n","* Method #1: simple split of the dataset with `my_train_test_split()`\n","* Method #2: `scikit-learn.model_selection.train_test_split`\n"]},{"cell_type":"code","metadata":{"id":"y7tbxjvNSnxN"},"source":["# Sample the entire dataset\n","X = df['carat'].values.reshape(-1,1)\n","y = df['price'].values\n","\n","# Method #1: Train/test split based on sequential sampling of the dataset\n","X_train, X_test, y_train, y_test = my_train_test_split(X, y, test_size=0.3)\n","\n","# Method #2: Train/test split with scikit-learn (random sampling)\n","#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g-VP1I6fzRsx"},"source":["# Plot the model prediction (no noise) as a smooth curve\n","X_model = [[0], [3]] # dependent variable, no noise\n","yhat_model = model.predict(X_model) # compute the model prediction\n","plt.plot(X_model, yhat_model, color='k', label=\"model\")\n","\n","# Plot the data (scatterplot)\n","plt.scatter(X_train, y_train, color=['red'], alpha=.1, label=\"training data\")\n","plt.scatter(X_test, y_test, alpha=.1, label=\"testing data\")\n","plt.legend();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nM9zyJNiCglO"},"source":["\n","Data visualization explains the difference between the two methods of train/test split.\n","\n","Discussion: The results are very sensitive to the way in which the train/test split is implemented with Method #1. This might be interpreted as overfitting. However, overfitting is not what you would expect with a large dataset (more than 50K samples) and a very simple linear-regression model. But data visualization reveals that the sensitivity with Method #1 is really a symptom of model bias combined with non-uniform sampling in the dataset. In fact, with a train/test split based on randomized sampling (Method #2), training and test performance are nearly identical.\n","\n","Note: To keep things simple, the analysis below doesn't use train/test splits."]},{"cell_type":"code","metadata":{"id":"3pWXl5Uw8HVj"},"source":["import numpy as np\n","\n","# Linear regression with polynomials is built-in with Seaborn\n","# With Scikit-Learn, you need to do it yourself.\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","# Add PolynomialFeatures of desired degree\n","# Inspect X to confirm the result (original series is in column #1\n","# Column #0 is full of ones (i.e., the intercept)\n","# To plot a line, use degree=1, in which case X_poly has 2 columns\n","poly = PolynomialFeatures(degree=5)\n","X_poly = poly.fit_transform(X)\n","\n","# Fit the model to the data\n","# Note: PolynomialFeatures adds the intercept (ones) to column 0\n","# hence fit_intercept=False, and the data gets moved to column 1\n","model = LinearRegression(fit_intercept=False)\n","model.fit(X_poly, y)\n","\n","# Plot the model prediction (no noise) as a smooth curve\n","X_model = np.arange(0, 5, .05).reshape(-1, 1) # dependent variable, no noise\n","X_model = poly.transform(X_model) # add the polynomials\n","yhat_model = model.predict(X_model) # compute the model prediction\n","plt.plot(X_model[:,1], yhat_model, color='k') # remember: column 1 is the dependent variable\n","\n","# Plot the data (scatterplot)\n","plt.scatter(X[:,0], y, alpha=.5)\n","plt.show();\n","\n","# Compute the model prediction at the data points\n","yhat = model.predict(X_poly)\n","\n","print('Explained variance: {:.2f}%'.format(100 * explained_variance_score(y, yhat)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lFqIVDS81C1w"},"source":["# Residuals\n","\n","* The simple linear regression model (above) clearly suffers from being oversimplified (high bias).\n","* Linear regression with a high-order polynomial model has problems as well.\n","    * The problems are worse where the data are sparse.\n","* Visualize the residuals to help assess model performance\n","    * In particular, check for \"structure\" in residuals that's inconsistent with random noise."]},{"cell_type":"code","metadata":{"id":"pGFNc_dRUS2v"},"source":["# Plot the residuals\n","plt.scatter(X[:,0], y - yhat, alpha=.5)\n","plt.plot([0, 5], [0,0], linestyle=\"dashed\", color=\"k\")\n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C95hqAYWWHMA"},"source":["* If you investigate different values for \"degree\" in PolynomialFeatures\n","    * Results:\n","    * degree = 1 is a linear fit\n","    * degree > 1 produces strange features where data are sparse\n","        * but the degree doesn't matter where data are dense\n","* Investigate the impact of sub-sampling scheme"]},{"cell_type":"markdown","metadata":{"id":"szeZuZf1zZP7"},"source":["# Logarithmic dependency?\n","\n","* Clearly there's structure in the residuals -- is it logarithmic?\n"]},{"cell_type":"markdown","metadata":{"id":"_9s3D9LZ2sYS"},"source":["# Log scales\n","\n","* Exponential structure suggests logarithmic scale transformation...\n","    * ...but `log(x)` isn't helpful in this case\n","* We can see that by transforming `x` ourselves with matplotlib"]},{"cell_type":"code","metadata":{"id":"3c806pTq3nqA"},"source":["# Log-x scaling for the independent variable (x = carat)\n","plt.scatter(df['carat'], df['price'], alpha=.1)\n","plt.gca().set_xscale('log');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7K7BDOmaaRKA"},"source":["* What about `log(y)`?\n","* Again, this can be visualized with matplotlib"]},{"cell_type":"code","metadata":{"id":"ItQZ7edojOpC"},"source":["# Log scaling for the dependent variable (y = price)\n","plt.scatter(df['carat'], df['price'], alpha=.1)\n","plt.gca().set_yscale('log');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ALNOfjvvaaUv"},"source":["* And log-log?"]},{"cell_type":"code","metadata":{"id":"xESdHelo4CxY"},"source":["# Data seem relatively linear with log-log scales\n","plt.scatter(df['carat'], df['price'], alpha=.1)\n","plt.gca().set_xscale('log')\n","plt.gca().set_yscale('log');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GB9NmoRd9y-6"},"source":["# Modeling logarithmic dependencies\n","\n","* log transformation of the data - create two new columns\n","* transform both feature & target variables for log-log regression\n","* perform linear regression with log-transformed variables"]},{"cell_type":"code","metadata":{"id":"hmWRQYG558Ob"},"source":["# Create a column with log(price) and a column with log(carat)\n","diamonds['log(price)'] = diamonds['price'].transform(np.log10)\n","diamonds['log(carat)'] = diamonds['carat'].transform(np.log10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CwnFoQJDkhhU"},"source":["# Linear regression with log-transformed data\n","df = diamonds\n","\n","X = df[['log(carat)']]\n","y = df['log(price)']\n","\n","# Fit the model to the data\n","model = LinearRegression()\n","model.fit(X, y)\n","\n","# Predict the data with the model\n","yhat = model.predict(X)\n","\n","# Assign model to a new column in the dataframe\n","df = df.assign(model_loglog = yhat)\n","\n","# Evaluate the performance\n","print('Explained variance: {:.2f}%'.format(100 * explained_variance_score(y, yhat)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MfHo6c8C4akg"},"source":["df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EZSO6kFis3DJ"},"source":["# Residuals with log-log scaling\n","\n","Is this better?\n","\n","Be careful about comparing explained variance -- we've rescaled the data.\n","\n","We need a way to assess the assumption of model + random noise. We'll further analyze the residuals."]},{"cell_type":"code","metadata":{"id":"F3G1GuPUnStv"},"source":["# Plot the residuals\n","X = df['log(carat)']\n","residuals = df['log(price)'] - df['model_loglog']\n","\n","plt.scatter(X, residuals, alpha=.5)\n","plt.plot([-.7, .7], [0,0], linestyle=\"dashed\", color=\"k\")\n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8EmVdgGOdck1"},"source":["# STOPPED HERE\n"]},{"cell_type":"code","metadata":{"id":"zmnlh752gFOy"},"source":["# Generate some synthetic data: linear model plus random noise\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","m = 100\n","\n","# Model\n","w0 = 3\n","w1 = 0.042\n","xs = np.arange(0, m * 100) / m\n","ys = w0 + w1 * xs\n","\n","# Noise\n","mu, sigma = 0, 0.1 # mean and standard deviation\n","random.seed(42) # for reproducibility\n","eps = [random.normalvariate(mu, sigma) for i in enumerate(xs)]\n","\n","# Data\n","data = ys + eps"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fl3EhqWJgMML"},"source":["# Visualization with matplotlib\n","plt.plot(xs, data,'o', label='data')\n","plt.plot(xs, ys, label='model')\n","plt.ylabel('y')\n","plt.xlabel('x')\n","plt.legend();"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yQBtlHRmgzpe"},"source":["# Linear regression with scikit-learn\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import explained_variance_score\n","\n","Xs = xs.reshape(-1,1)\n","\n","print(xs.shape)\n","print(Xs.shape)\n","\n","# Fit the model to the data\n","model = LinearRegression()\n","model.fit(Xs, data)\n","\n","# Evaluate the performance\n","ys_hat = model.predict(Xs)\n","print('Explained variance: {:.2f}%'.format(100 * explained_variance_score(data, ys_hat)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P0QMeCo1kI1g"},"source":["# Visualization with matplotlib\n","plt.plot(xs, data,'o', label='data')\n","plt.plot(xs, ys_hat,'--', label='prediction')\n","plt.plot(xs, ys, label='model')\n","plt.ylabel('y')\n","plt.xlabel('x')\n","plt.legend();"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7c2mJ-7Gkixo"},"source":["plt.plot(xs, data - ys_hat, 'o', label='residual')\n","plt.ylabel('y')\n","plt.xlabel('x')\n","plt.legend();"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MKVeaYmolAHj"},"source":["plt.hist(data);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LajOm6BPlGm7"},"source":["plt.hist(data - ys_hat);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_OGwFdhG3eED"},"source":["# Visualizing data distributions\n","\n","* Statistical assessment of data and residuals\n","* Histograms with matplotlib\n","    * assessing the assumption of random noise\n","    * looking for [skewness](https://en.wikipedia.org/wiki/Skewness) or other departures from normal distribution\n","    * styling with the matplotlib API\n","* Clearly, diamond \"price\" is right skewed"]},{"cell_type":"code","metadata":{"id":"RQqhWpjE1Xww"},"source":["from matplotlib.patches import Rectangle\n","\n","# Histogram of price is highly skewed (positive or right skew)\n","plt.hist(diamonds['price']);\n","\n","# Histogram styling. It can be done, but...\n","ax = plt.gca()\n","lines = ax.get_lines()\n","children = ax.get_children()\n","print('axes:', ax)\n","print('lines (histograms do not use lines):', lines)\n","print('children:', type(children), len(children))\n","print('children[0]:', children[0])\n","[child.set_edgecolor('k') for child in children if isinstance(child, Rectangle)];"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v5azNJxNgjz2"},"source":["# Convenience function for styled histograms\n","def styled_histogram(series):\n","    plt.hist(series)\n","    ax = plt.gca()\n","    children = ax.get_children()\n","    [child.set_edgecolor('k') for child in children if isinstance(child, Rectangle)];"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t8almtvnuPvF"},"source":["# Distribution of log(price)\n","\n","* Compare distributions of price & log(price)\n","* Compare distributions residuals from log-log model"]},{"cell_type":"code","metadata":{"id":"GGFRD9sv8doz"},"source":["# Distribution of log(price) is much more symmetric\n","styled_histogram(df['log(price)'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hhqqjnnwkoXa"},"source":["# Plot the histogram of the residuals from the linear regression of log-log data\n","residuals = y - yhat\n","styled_histogram(residuals)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b95LuUpGln-Y"},"source":["# Q-Q plot\n","\n","The distributions are symmetric, but are they \"good\"? And what is a metric for \"good\"?\n","\n","* Quantiles (Q-Q plot) of data compared with theoretical probability distributions (probability plot)\n","* [Quantiles](https://en.wikipedia.org/wiki/Quantile) are values dividing a probability density into equal areas\n","* Using the scipy.stats library, default theoretical distribution is standard normal\n","* Using statsmodels, the default distribution is scipy.stats.distributions.norm\n","\n","### references\n","\n","* [scipy reference docs](https://docs.scipy.org/doc/scipy/reference/) -- scipy.org\n","* [scipy.stats.probplot()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html) -- scipy.org\n","\n"]},{"cell_type":"code","metadata":{"id":"Ftge8YF3lpxq"},"source":["from scipy import stats\n","\n","# Q-Q plot (data vs standard normal)\n","# stats.probplot(offsets[:,1], plot=plt);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_QIvausDp1sH"},"source":["## Create a convenience function for a styled Q-Q plot"]},{"cell_type":"markdown","metadata":{"id":"BFcB7_AUzjly"},"source":["### styling\n","\n","`stats.probplot()` uses 2 plots types (line and scatterplot) with harsh colors\n","\n","* [Q-Q plot](https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot) -- wikipedia\n","* [scipy.stats.probplot](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html) API reference docs - scipy.org\n","    * plots quantiles of data against Normal distribution (default)\n","* Q: Can I change the colors?\n","* A: Yes, but the `scipy.stats` API docs don't speak to this.\n","    * You need to use the matplotlib API"]},{"cell_type":"code","metadata":{"id":"jKlixhd0oEqZ"},"source":["def qqplot(data):\n","    stats.probplot(data, plot=plt)\n","\n","    # Change the styling\n","    ax = plt.gca()\n","    lines = ax.get_lines() # there are two -- dots and line\n","    lines[0].set_markerfacecolor(\"steelblue\")\n","    lines[0].set_markeredgecolor(\"steelblue\")\n","    lines[0].set_alpha(0.5)\n","    lines[1].set_color('k');\n","\n","styled_histogram(residuals)\n","plt.show()\n","\n","qqplot(residuals)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SwOX91F1CBIA"},"source":["# Bad residuals -- linear scales\n","\n"]},{"cell_type":"code","metadata":{"id":"hKQUZvCEqfvW"},"source":["X_linear = df[['carat']]\n","y_linear = df['price']\n","\n","# Fit the model to the data\n","model = LinearRegression()\n","model.fit(X_linear, y_linear)\n","\n","# Evaluate the performance\n","yhat_linear = model.predict(X_linear)\n","residuals_linear = y_linear - yhat_linear\n","print('Explained variance: {:.2f}%'.format(100 * explained_variance_score(y_linear, yhat_linear)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CGd7x_qUB7G2"},"source":["styled_histogram(residuals_linear)\n","plt.show();\n","\n","qqplot(residuals_linear)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aY8lwxkqZG-o"},"source":["# Q-Q plot with statsmodels\n","import statsmodels.api as sm\n","sm.qqplot(residuals_linear, fit=True, line=\"45\")\n","\n","# Change the styling\n","ax = plt.gca()\n","lines = ax.get_lines() # there are two -- dots and line\n","lines[0].set_markerfacecolor(\"steelblue\")\n","lines[0].set_markeredgecolor(\"steelblue\")\n","lines[0].set_alpha(0.5)\n","lines[1].set_color('k');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MWG6XsRpsHY1"},"source":["# Q-Q plot with statsmodels\n","import statsmodels.api as sm\n","sm.qqplot(residuals, fit=True, line=\"45\")\n","\n","# Change the styling\n","ax = plt.gca()\n","lines = ax.get_lines() # there are two -- dots and line\n","lines[0].set_markerfacecolor(\"steelblue\")\n","lines[0].set_markeredgecolor(\"steelblue\")\n","lines[0].set_alpha(0.5)\n","lines[1].set_color('k');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uvGnjJr1_mb3"},"source":["# EXERCISE\n","\n","Visualize the relationship between residuals in log-log model and cut, clarity, color\n","* First, show students how to do a boxplot\n","* Then let them filter the residuals and visualize relationship"]},{"cell_type":"markdown","metadata":{"id":"D5GPS2avxwPX"},"source":["# Visualizing distributions with boxplots\n","\n","* matplotlib has a boxplot capability (you don't have to use seaborn)\n","* [boxplot demo](https://matplotlib.org/stable/gallery/pyplots/boxplot_demo_pyplot.html)\n","* [matplotlib.pyplot.boxplot](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.boxplot.html) API reference docs"]},{"cell_type":"code","metadata":{"id":"HY4smc8wxx7_"},"source":["# Random data\n","np.random.seed(42)\n","spread = np.random.rand(50) * 100 # random numbers (mean 50) (50 of them)\n","center = np.ones(25) * 50  # constants = 50 (25 of them)\n","flier_high = np.random.rand(10) * 100 + 100 # 10 extra large values\n","flier_low = np.random.rand(10) * -100 # 10 extra small values\n","data = np.concatenate((spread, center, flier_high, flier_low))\n","assert data.shape == (95,)\n","\n","# Create the boxplot\n","fig1, ax1 = plt.subplots()\n","ax1.set_title('Basic Plot')\n","ax1.boxplot(data);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IZuBg9OC0SIZ"},"source":["# Add more random data \n","spread = np.random.rand(50) * 100\n","center = np.ones(25) * 40\n","flier_high = np.random.rand(10) * 100 + 100\n","flier_low = np.random.rand(10) * -100\n","d2 = np.concatenate((spread, center, flier_high, flier_low))\n","\n","# Create an array of dataframes, with one dataframe for each subplot\n","my_list = [data, d2, d2[::2]] # This works, but boxplot wants an array\n","# my_array = np.array(my_list)  # This conversion issues a deprecation warning\n","my_array = np.array(my_list, dtype=\"object\") # This is the right way\n","\n","fig7, ax7 = plt.subplots()\n","ax7.set_title('Multiple Samples with Different sizes')\n","ax7.boxplot(my_array); # With \"my_list\", boxplot issues a deprecation warning\n","\n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8ltEhOghLlcd"},"source":["# Other candidate predictors"]},{"cell_type":"code","metadata":{"id":"GQthitn7RjsS"},"source":["print('Unique cuts:', df['cut'].unique())\n","print('Unique colors:', df['color'].unique())\n","print('Unique clarities:', df['clarity'].unique())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UfHEYwjQ3zqg"},"source":["df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h7Ct0e-j3VSE"},"source":["# Another visualization of the same data -- Q: Is this more informative?\n","# It exhibits the non-intuitive relationship between price & cut that we've seen before\n","import seaborn as sns\n","\n","cuts = [\"Fair\", \"Good\", \"Very Good\", \"Premium\", \"Ideal\"]\n","\n","sns.catplot(data=df, x=\"cut\", y=\"price\", kind=\"point\", order=cuts);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GRePdnYP5zX8"},"source":["# Likewise for log(price) -- relationship to \"cut\" is hard to understand\n","sns.catplot(data=df, x=\"cut\", y=\"log(price)\", kind=\"point\", order=cuts);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eAY4uqbL5fHQ"},"source":["# Box plot exhibits similar relationship\n","sns.catplot(data=df, x=\"cut\", y=\"log(price)\", kind=\"box\", order=cuts);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eWMRA1ff5q9u"},"source":["df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EL3h0pZlhOSs"},"source":["## Comparing residuals of linear regression\n","\n","Visualize the relationship between diamond \"cut\" and residuals of linear model (after log-log transormation).\n","\n","In contrast with visualization above, residuals exhibit the expected relationahip to cut:\n","\n","Higher cut quality is asociated with larger log(price) "]},{"cell_type":"code","metadata":{"id":"SqBdaDi66Gix"},"source":["df['resid'] = df['log(price)'] - df['model_loglog']\n","df\n","\n","sns.catplot(data=df, x=\"cut\", y=\"resid\", kind=\"box\", order=cuts);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NR4hhNJSx82I"},"source":["# Linear regression with categorical features - one-hot encoding\n","\n","* You *can* do linear regression with categorical features\n","  * one-hot encoding of categorical features\n","  * use `drop=\"first\"` (to avoid colinear inputs)\n","* predict log(price)\n","* add log(carat) as a feature\n","* [6.3 Preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html) (sklearn.preprocessing package) -- scikit-learn.org\n","    * [6.3.4 Encoding categorical features](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features) -- scikit-learn.org\n","    * One-hot encoding of categorical features (indicator variables)\n","* Documentation discusses `drop=\"feature\"` parameter that avoids colinear inputs\n","    * Colinear inputs would cause non-regularized linear regression to fail"]},{"cell_type":"markdown","metadata":{"id":"Ue7vxzgOYN7h"},"source":["### modeling the residuals\n","\n","* the next cell uses categorical features to model the residuals of linear regression with log-log transformation\n","  * first model -- log(price) vs log(carat)\n","  * this is a type of \"forward selection\""]},{"cell_type":"code","metadata":{"id":"NzyzFzE083vC"},"source":["# Multivariate linear regression of residuals from log-log model\n","# Uses one-hot encoding of the categorical features\n","from sklearn import preprocessing\n","from scipy.sparse import hstack\n","from sklearn.metrics import explained_variance_score\n","\n","enc = preprocessing.OneHotEncoder(drop=\"first\")\n","#enc = preprocessing.OneHotEncoder()\n","\n","y = df['resid']\n","X = df.loc[:, ['cut', 'color', 'clarity']].values\n","\n","X = enc.fit_transform(X)\n","\n","model = LinearRegression()\n","model.fit(X, y)\n","\n","yhat = model.predict(X)\n","\n","print('Explained variance (before): {:.2f}%'.format(100 * explained_variance_score(df['log(price)'], df['model_loglog'])))\n","print('Explained variance (after): {:.2f}%'.format(100 * explained_variance_score(df['log(price)'], df['model_loglog'] + yhat)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BqeXOvyrY_QC"},"source":["# Simultaneous multivariate regression of log(price)\n","\n","* rather than a sequence of models this model considers all variables simultaneously\n","* explained variance is larger than the result from the cell above"]},{"cell_type":"code","metadata":{"id":"Moxbi0wx_jKv"},"source":["# Multivariate linear regression with one-hot encoding of categorical features\n","# This model includes log(carat) as a variable in the model\n","from sklearn import preprocessing\n","from scipy.sparse import hstack\n","from sklearn.metrics import explained_variance_score\n","\n","enc = preprocessing.OneHotEncoder(drop=\"first\")\n","#enc = preprocessing.OneHotEncoder()\n","\n","y = df['log(price)']\n","X = df.loc[:, ['cut', 'color', 'clarity']].values\n","\n","X = enc.fit_transform(X)\n","\n","print(type(X))\n","print(\"X.shape:\", X.shape)\n","print(len(enc.categories_))\n","print(enc.categories_)\n","print('df[log(carat)].shape', df['log(carat)'].values.reshape(-1,1).shape)\n","\n","X = hstack((X, df[['log(carat)']].values))\n","\n","model = LinearRegression()\n","model.fit(X, y)\n","\n","yhat = model.predict(X)\n","\n","print('Explained variance (before): {:.2f}%'.format(100 * explained_variance_score(y, df['model_loglog'])))\n","print('Explained variance (after): {:.2f}%'.format(100 * explained_variance_score(y, yhat)))"],"execution_count":null,"outputs":[]}]}