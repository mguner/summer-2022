{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"11-NLP.ipynb","provenance":[{"file_id":"1zc6iKgyf3VlFLrpnBtnInZo_LJ4ys-nW","timestamp":1627747931438}],"collapsed_sections":[],"authorship_tag":"ABX9TyMetQKszSbVfFosDeVyznnO"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"aCMLFy8A43rJ"},"source":["<table align=\"center\">\n","   <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ds5110/summer-2021/blob/master/11-NLP.ipynb\">\n","<img src=\"https://github.com/ds5110/summer-2021/raw/master/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n","</table>\n"]},{"cell_type":"markdown","metadata":{"id":"NIRgkpe_JIAD"},"source":["# 11 -- Natural Language Processing (NLP)\n","\n","* Structuring and cleaning text data\n","* EDA using term frequency (Jane Austen novels)\n","* Sentiment analysis (IMDb reviews)\n","* Text classification (Naive Bayes with newsgroup documents)\n","* Unsupervised topic modeling (LSA Brown corpus, LDA IMDb reviews)\n","\n","# Data\n","\n","Some of the sources we'll be looking at...\n","\n","* [Jane Austen novels](https://github.com/juliasilge/janeaustenr) -- github\n","* [Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus) of 500 text samples in American English\n","  * The first million+ word corpus collected from various sources\n","* [20 Newsgroups dataset](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#loading-the-20-newsgroups-dataset) -- scikit-learn.org\n","  * a collection of ~20k newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups\n","  * the dataset has become a popular for text classification and text clustering.\n","* Tweets"]},{"cell_type":"markdown","metadata":{"id":"HFHCno7o0hSb"},"source":["# Jane Austen\n","\n","Analysis of her books.\n","\n","### Reading\n","\n","* Chapter 3 of [Tidy Text Mining](https://www.tidytextmining.com/tfidf.html)\n","\n","### References\n","\n","* [Jane Austen's books](https://github.com/juliasilge/janeaustenr) in R (Julia Silge) -- github.com\n","* [austen_books.csv](https://github.com/ds5110/summer-2021/raw/master/data/austen_books.csv) for DS 5110 -- github"]},{"cell_type":"code","metadata":{"id":"3oo2WyN2on4J"},"source":["# Load the data from github\n","import pandas as pd\n","import numpy as np\n","\n","url = \"https://github.com/ds5110/summer-2021/raw/master/data/austen_books.csv\"\n","df = pd.read_csv(url, index_col=False).drop('Unnamed: 0', axis=1)\n","\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JMFqU4rNYyC2"},"source":["# Clean and inspect the data\n","\n","* the data: Jane Austen's books as a single dataframe\n","* **document** (definition): the text from a single row in the dataframe\n","  * a tweet would be a document if we're analyzing tweets\n","  * likewise, an IMDb movie review would be a document\n","  * with this definition, a Jane Austen book is a collection of documents"]},{"cell_type":"code","metadata":{"id":"oTyi1kBJ1tnY"},"source":["# List the book names\n","books = df['book'].unique()\n","[print(book) for book in books];"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6QZOE8OIpS6d"},"source":["# Remove rows with missing data\n","n_rows = df.shape[0]\n","df = df.dropna()\n","\n","n_clean = df.shape[0]\n","print('{:,} rows dropped = {:.2f}%'.format(n_rows - n_clean, 100 * (1 - n_clean / n_rows)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bbr7MeEzl9hA"},"source":["# Extract and inspect the text for a book (as a pandas Series)\n","book = df[df['book'] == books[0]]['text']\n","first_line = book[0]\n","print(type(book))\n","print('book.shape:', book.shape)\n","print('first_line = book[0]:', first_line)\n","print('type(first_line)', type(first_line))\n","print('book[15]:', book[15])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-j7gyYIBYkcn"},"source":["# Bag of words -- tf -- sparsity\n","\n","* The bag-of-words model represents text as numerical feature vectors as follows:\n","  * Step 1: create a vocabulary of unique tokens\n","  * Step 2: construct a feature vector for each document with counts of word occurence\n","* Unique words in each document typically represent only a small subset of all the words in the bag-of-words vocabulary\n","* Therefore, feature vectors will be sparse (i.e., mostly zeros)"]},{"cell_type":"markdown","metadata":{"id":"3YYCDCwCh2Mr"},"source":["## Classes for text analysis\n","\n","* [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) -- Converts collection of text documents to a sparse matrix of token counts\n","* [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) -- Transforms count matrix to a normalized tf **or** tf-idf representation\n","* [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) -- equivalent to CountVectorizer followed by TfidfTransformer\n","\n"]},{"cell_type":"code","metadata":{"id":"PkcgXE-Z3-4C"},"source":["# Load the sklearn classes for text analysis\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","from sklearn.feature_extraction.text import TfidfVectorizer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0qvDLbsry1MS"},"source":["# A toy demo"]},{"cell_type":"code","metadata":{"id":"Wl6tfOn0vqgX"},"source":["# Use CountVectorizer to create a bag of words (matrix of feature counts)\n","count = CountVectorizer()\n","docs = np.array(['The sun is shining',\n","                 'The weather is sweet',\n","                 'The sun is shining, the weather is sweet,'\n","                 'and one and one is two'])\n","bag = count.fit_transform(docs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yh4HGy3YzBYT"},"source":["# The vocabulary\n","count.vocabulary_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"br-jNovgzVHe"},"source":["# The bag\n","bag"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3F9xIilUzkUL"},"source":["print(type(bag))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_PX6NDM4ztIp"},"source":["### Sparse matrices\n","\n","CountVectorizer returns `csr_matrix` -- yet another matrix format!?\n","\n","* [scipy.sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html)\n","  * Sparse matrices support addition, subtraction, multiplication, division, and matrix power\n","  * [discussion of sparsity](https://scikit-learn.org/stable/modules/feature_extraction.html#sparsity) -- scikit-learn.org\n","* Advantages of the scipy CSR (Compressed Sparse Row) format:\n","  * efficient arithmetic operations CSR + CSR, CSR * CSR, etc.\n","  * efficient row (document) slicing\n","  * fast matrix-vector products\n","* Disadvantages:\n","  * slow column slicing operations\n","    * consider [CSC (Compressed Sparse Column) matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html) instead\n","  * changes to the sparsity structure are expensive\n","    * consider [LIL (List of Lists)](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html) or [DOK (Dictionary of Keys)](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.dok_matrix.html) alternatives"]},{"cell_type":"code","metadata":{"id":"kWto_Dxc15Rn"},"source":["# Note: vocabulary is alphabetical\n","[print(doc) for doc in docs]\n","print('\\n', count.vocabulary_)\n","print('\\n', bag.toarray())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V8rM-ieFyw9S"},"source":["## Analyzing Jane Austen's books"]},{"cell_type":"code","metadata":{"id":"rBqdUT2EhM10"},"source":["# Use CountVectorizer and TfidfTransformer to compute the raw term frequencies\n","\n","# NOTE: a \"book\" in this case is a pandas Series with length ~10K (number of lines)\n","count = CountVectorizer()\n","sparse_matrix = count.fit_transform(book)\n","\n","print(type(sparse_matrix))\n","print('sparse_matrix.shape:', sparse_matrix.shape)\n","\n","# Then compute the raw term frequencies (with TfidfTransformer)\n","# These are term frequencies for each document (row in the matrix)\n","# There are ~10K lines, and ~6K terms (features)\n","tf = TfidfTransformer(use_idf=False).fit_transform(sparse_matrix)\n","\n","# TfidfTransformer returns \n","print(type(tf))\n","print(tf.shape)\n","tf.toarray()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N_GakG7zhZ_1"},"source":["first_line = book[0]\n","first_line"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6NvXF0tnmvpq"},"source":["# Convert the first line of the book (a string) to an array of words (features)\n","first_line.lower().split()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dKEk_WbNkVOY"},"source":["# Get indices for the words in the first line\n","# Note that you need both .lower() & .split())\n","#   Without \".lower()\", first_line throws a KeyError because it's all caps\n","#   Without \".split()\", you get a long string that doesn't distinguish words\n","first_line_indices = [count.vocabulary_[word] for word in first_line.lower().split()] # need .lower()\n","first_line_indices"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"slc5GHASnV2f"},"source":["# Verify that the indices above agree with the first row of the sparse_matrix\n","# Note: the indices are the same as above, but the order has been lost\n","a = sparse_matrix[0, :] > 0\n","a = a.toarray()[0]\n","feature_indices = [i for i, x in enumerate(a) if x]\n","feature_indices"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V3bpPPNerR0h"},"source":["# Invert the transform\n","# Note: the order is incorrect, and there are no caps or punctuation\n","feature_names = count.get_feature_names()\n","[feature_names[i] for i in feature_indices]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4yTX9rTdM9Za"},"source":["# raw term frequencies -- tf(t,d) -- for an entire book\n","\n","Plot the raw term (t = term = word) frequencies (histogram) for a book (here we redefine d = document = book)"]},{"cell_type":"code","metadata":{"id":"sKvcy0aOnDEc"},"source":["# Compute term frequencies for an entire book\n","\n","# Redefining sparse_matrix here (same as above, just for clarity)\n","# A book contains multiple rows\n","# Sparse matrix dimension is documents-by-vocabulary size: (10596, 6305)\n","book = df[df['book'] == books[0]]['text']\n","count = CountVectorizer()\n","sparse_matrix = count.fit_transform(book)\n","\n","# First, convert the sparse array to an ndarray, then term sums\n","# .toarray() -- returns dense ndarray\n","# .sum(axis=0) -- returns sum over rows for each column (term)\n","a = sparse_matrix.toarray().sum(axis=0)\n","print(sparse_matrix.shape)\n","print(a.shape)\n","\n","# .argsort() -- returns indicies that would sort the array\n","sorted_indices = np.argsort(-a)\n","sorted_count = [a[i] for i in sorted_indices]\n","sorted_features = [feature_names[i] for i in sorted_indices]\n","print('Number of features:', len(sorted_features))\n","print('First 10 counts:', sorted_count[:10])\n","print('First 10 features:', sorted_features[:10])\n","print('Last 10 counts:', sorted_count[-10:])\n","print('Last 10 features:', sorted_features[-10:])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"87EAl9cWBNpz"},"source":["# Demo bar chart\n","import matplotlib.pyplot as plt\n","\n","a = ['a', 'b', 'c']\n","b = [10,20,30]\n","plt.bar(a,b);  # width=1 "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YcH1Af-eKVcx"},"source":["# Plot the raw term frequencies as a bar chart (most frequent)\n","plt.subplots(1,1,figsize=(12,5))\n","plt.bar(sorted_features[:25], sorted_count[:25], \n","        width=1, alpha=.5, edgecolor='black')\n","plt.xticks(rotation=45);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lu4eI30DmkV-"},"source":["# Plot the raw term frequencies as a bar chart (least frequent)\n","plt.subplots(1,1,figsize=(12,5))\n","plt.bar(sorted_features[-25:], sorted_count[-25:], \n","        width=1, alpha=.5, edgecolor='black')\n","plt.xticks(rotation=45);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FajdgvlCNjb7"},"source":["# Zipf's law\n","\n","Empirical relation between term frequency and inverse of rank in a list of word frequencies\n","\n","$$\n","\\mathrm{tf} = \\frac{\\gamma}{\\mathrm{rank}}\n","$$\n","or\n","$$\n","\\log(\\mathrm{tf}) = \\log(\\gamma) - \\log(\\mathrm{rank})\n","$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VByxUKfB5bLL"},"source":["Recall\n","\n","$$\n","a = \\frac{\\gamma}{b} = \\gamma b^{-1}\n","$$\n","\n","$$\n","\\log a = \\log \\left(\\gamma b^{-1}\\right) \n","= \\log \\gamma + \\log b^{-1} = \\log \\gamma - \\log b\n","$$"]},{"cell_type":"code","metadata":{"id":"gaaWPwiYzB_f"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","a = np.arange(.1, 10, .1)\n","gamma = -1.0\n","b = a**gamma\n","\n","fig, ax = plt.subplots(1,2, figsize=(12,5))\n","ax[0].plot(a,b)\n","ax[1].loglog(a,b)\n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f8cNA5r3QK2T"},"source":["n = len(sorted_count)\n","print(n)\n","rank = np.arange(1, n + 1)\n","\n","fig, ax = plt.subplots(1,2, figsize=(12,5))\n","ax[0].plot(rank, 1.5e+4 / rank)\n","ax[0].plot(rank, sorted_count)\n","ax[0].set_xlabel('rank')\n","ax[0].set_ylabel('frequency')\n","ax[0].set_title('tf (linear scales)')\n","\n","ax[1].loglog(rank, 1.5e+4 / rank)\n","ax[1].loglog(rank, sorted_count)\n","ax[1].set_xlabel('rank')\n","ax[1].set_title('tf (log scales)')\n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DdOJu1kgqPnj"},"source":["# tf-idf\n","\n","tf-idf attempts to find the words that are common and important, but not so common as to be unimportant.\n","\n","$$\n","\\mathrm{tf\\mathrm{-}idf}(t, d) = \\mathrm{tf}(t,d) * \\mathrm{idf}(t)\n","$$\n","Typically, if $n$ is the number of documents and df(t) is the number of documents containing the term $t$, then\n","$$\n","\\mathrm{idf}(t) = \\log\\left( \\frac{n}{\\mathrm{df}(t)} \\right)\n","$$\n","* If a term $t$ shows up in all documents, df(t) = n, idf(t) = 0\n","* If a term shows up in no documents, idf = infinity (i.e., divide-by-0)\n","* In practice, idf(t) is maximum when $t$ shows up in 1 document (the minimum)\n","* See [TfidfTransformer reference docs](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) for variations that avoid divide-by-zero\n"]},{"cell_type":"code","metadata":{"id":"nupmGqEydvOW"},"source":["# EXERCISE: Plot idf -- it is linear in log-log plot, but smaller range than tf\n","from sklearn.pipeline import Pipeline\n","\n","transformer = TfidfTransformer()\n","tfidf = transformer.fit_transform(sparse_matrix)\n","print('tfidf.shape:', tfidf.toarray().shape)\n","\n","sorted_idf = [transformer.idf_[i] for i in sorted_indices]\n","\n","fig, ax = plt.subplots(1,2, figsize=(12,5))\n","ax[0].plot(rank, sorted_idf)\n","ax[0].set_title('sorted idf (linear scales)')\n","ax[1].loglog(rank, sorted_idf)\n","ax[1].set_title('sorted idf (log scales)')\n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T9hpkFA3qIvj"},"source":["# Corpus: Jane Austen novels\n","\n","For this analysis, document = book, corpus = collection of books.\n","\n","We'll see an example where a document is an IMDb review.  With twitter analyses, it's common that document = tweet."]},{"cell_type":"code","metadata":{"id":"DXe2zFHYkvFG"},"source":["print(len(sorted_count))\n","print(type(sorted_count))\n","a = np.asarray(sorted_count)\n","print(a.shape)\n","b = np.asarray(sorted_idf)\n","max(a*b)\n","\n","plt.loglog(rank, a*b)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HlDf0vZNMF48"},"source":["# Convert the original dataframe into a list of books (corpus)\n","# Each book is a string (document in the corpus)\n","corpus = []\n","for title in books:\n","  book = df[df['book'] == title]['text']\n","  book = \" \".join(book)\n","  corpus.append(book)\n","print(len(corpus)) # number of books (6)\n","print(corpus[0]) # first book"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XwJtNfqKfITc"},"source":["# stop words\n","\n","* Stop words like “and”, “the”, “him” are presumed to be unimportant.\n","* They may be removed.\n","* However, they may be useful for prediction, such as in classifying writing style or personality\n","* There are various ways to deal with stop words\n","* [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) has built-in `stop_words` with acknowledged \"issues\" -- scikit-learn.org\n","* [stop-words](https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words) discussion -- scikit-learn.org\n","\n","# tf-idf\n","\n","* default keeps all words, even those that appear in all documents\n","* Setting `max_df = 5/6` will ignore words appear in all documents (i.e., df = 6/6)\n","* Set `stop_words=\"english\"` will used canned stop words (overrides `max_df`)\n","* Recall: [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) is equivalent to CountVectorizer followed by TfidfTransformer\n","\n","### EXERCISE: In the next cell, try each vectorizer line in succession"]},{"cell_type":"code","metadata":{"id":"Mvu7iIrJMsZr"},"source":["# Plot tfidf vs rank...compare each of the 3 ways for dealing with stop-words\n","\n","vectorizer = TfidfVectorizer(stop_words=\"english\") # this will cause altorithm to ignore max_df\n","#vectorizer = TfidfVectorizer() # no stop words and the \"1\" added to df (above) allows terms that appear in all documents\n","vectorizer = TfidfVectorizer(max_df = 5/6) # ignore words that appear in all documents\n","\n","# sparse_matrix & feature_names defined here, and used below\n","sparse_matrix = vectorizer.fit_transform(corpus)\n","feature_names = vectorizer.get_feature_names()\n","\n","# Get highest tf-idf words in the first book\n","tfidf = sparse_matrix[0,:].toarray()[0]\n","print('tfidf:', tfidf[:10])\n","\n","# Sort tfidf from large to small (default sort is increasing)\n","sorted_indices = np.argsort(-tfidf) # these are the indices of the sort\n","sorted_tfidf = [tfidf[i] for i in sorted_indices] # this is the sorted array\n","sorted_features = [feature_names[i] for i in sorted_indices] # features sorted by tfidf\n","\n","print('sorted tfidf:', sorted_tfidf[:10])\n","print('sorted features:', sorted_features[:10])\n","\n","plt.subplots(1,1,figsize=(12,5))\n","plt.bar(sorted_features[:25], sorted_tfidf[:25], \n","        width=1, alpha=.5, edgecolor='black')\n","plt.title(title)\n","plt.xticks(rotation=45);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PDwfApYghXbI"},"source":["# Repeat analysis above, except for all the books\n","\n","# Define a convenience function for plotting sorted features\n","def plot_book(sorted_features, sorted_tfidf, ax, title, color):\n","    ax.bar(sorted_features[:25], sorted_tfidf[:25], \n","            width=1, alpha=.5, color=color, edgecolor='black')\n","    ax.set_title(title)\n","    ax.set_xticklabels(sorted_features[:25], rotation=45, ha='right');\n","\n","# Analyze and plot one book at a time\n","n = len(books)\n","fig, ax = plt.subplots(n, 1, figsize=(12,5 * n))\n","plt.tight_layout(pad=5)\n","colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n","for i in range(0, len(books)):\n","   tfidf = sparse_matrix[i,:].toarray()[0]\n","\n","   sorted_indices = np.argsort(-tfidf)\n","   sorted_tfidf = [tfidf[i] for i in sorted_indices]\n","   sorted_features = [feature_names[i] for i in sorted_indices]\n","\n","   plot_book(sorted_features, sorted_tfidf, ax[i], books[i], colors[i])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H2PBI7GatCMW"},"source":["\n","# N-grams\n","\n","EXERCISE: Try ngram_range = (1,2) and (2,2)"]},{"cell_type":"code","metadata":{"id":"6I1kYSOTDzKk"},"source":["# Plot tfidf vs rank...compare each of the 3 ways for dealing with stop-words\n","vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(2, 2)) # this will cause altorithm to ignore max_df\n","#vectorizer = TfidfVectorizer(ngram_range=(2, 2)) # no stop words and the \"1\" added to df (above) allows terms that appear in all documents\n","#vectorizer = TfidfVectorizer(max_df = 5/6, ngram_range=(2, 2)) # ignore words that appear in all documents\n","\n","# sparse_matrix & feature_names defined here, and used below\n","sparse_matrix = vectorizer.fit_transform(corpus)\n","feature_names = vectorizer.get_feature_names()\n","print('len(feature_names):', len(feature_names))\n","print('some feature_names:', feature_names[0:10]) # some (not all) are 2-grams"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gfjeJIBN9pLD"},"source":["n = len(books)\n","fig, ax = plt.subplots(n, 1, figsize=(13, 5 * n))\n","plt.tight_layout(pad=7)\n","colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n","for i in range(0, len(books)):\n","   tfidf = sparse_matrix[i,:].toarray()[0]\n","\n","   sorted_indices = np.argsort(-tfidf)\n","   sorted_tfidf = [tfidf[i] for i in sorted_indices]\n","   sorted_features = [feature_names[i] for i in sorted_indices]\n","\n","   plot_book(sorted_features, sorted_tfidf, ax[i], books[i], colors[i])"],"execution_count":null,"outputs":[]}]}