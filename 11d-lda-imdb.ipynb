{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"11d-lda-imdb.ipynb","provenance":[{"file_id":"1nOkjtnKPZTNpaol0d83PDVyq4l1h6Vl4","timestamp":1627983411371},{"file_id":"1DJzpqGgQWgpSDDeyv2LCRG1uxdUDOsOw","timestamp":1615305372570}],"collapsed_sections":[],"authorship_tag":"ABX9TyMtpXEQqzs9eWT4mJ/t5Gsg"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"kw-b4WI863DZ"},"source":["<table align=\"center\">\n","   <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ds5110/summer-2021/blob/master/11d-lda-imdb.ipynb\">\n","<img src=\"https://github.com/ds5110/summer-2021/raw/master/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n","</table>\n"]},{"cell_type":"markdown","metadata":{"id":"X2B0SXxHuBI9"},"source":["# 11d -- Latent Dirichlet Allocation (LDA) of the IMDb dataset\n","\n","Unsupervised LDA for topic modeling of the IMDb dataset\n","\n","Reference: Raschka's [ch08.ipynb](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb) -- github\n"]},{"cell_type":"markdown","metadata":{"id":"RECeKwSb7YSU"},"source":["* Latent Dirichlet Allocation (LDA) is a unsupervised generative statistical modeling technique\n","* With LDA, documents are organized into groups\n","  * Each document is a mixture of topics, and each word's presence is attributable to one of the document's topics\n","  * Topics are defined by groups of words that appear together frequently across documents\n","  * Given a bag-of-words as input, LDA decomposes the data into two matrices:\n","    * document-to-topic matrix\n","    * word-to-topic matrix\n","  * LDA is a generalization of probablistic Latent Semantic Analysis (pLSA)\n","    * LDA can be used to create synthetic documents\n","    * the synthetic documents share the same statistical properties of the training data\n","    * that's what is meant by generative\n","  * pLSA builds upon LSA by using a probabilistic multinomial distrubition to model document-word co-ocurrence\n","    * pLSA is not generative in the same sense as LDA\n","  * LSA is closely related to PCA\n","    * LDA is related to PCA, in that the matrix decomposition of the data optimizes a measure of error in the data representation.\n","* The number of topics is a hyperparameter of the analysis, akin to the dimensionality reduction that we've seen PCA."]},{"cell_type":"code","metadata":{"id":"yu3V0zuPwq6N"},"source":["# Read movie reviews from CSV in Raschka's github repo\n","# This cell replaces cells 2, 3 & 4\n","import os\n","import sys\n","import time\n","import pandas as pd\n","import urllib.request\n","\n","def reporthook(count, block_size, total_size):\n","    global start_time\n","    if count == 0:\n","        start_time = time.time()\n","        return\n","    duration = time.time() - start_time\n","    progress_size = int(count * block_size)\n","    speed = progress_size / (1024.**2 * duration)\n","    percent = count * block_size * 100. / total_size\n","\n","    sys.stdout.write(\"\\r%d%% | %d MB | %.2f MB/s | %d sec elapsed\" %\n","                    (percent, progress_size / (1024.**2), speed, duration))\n","    sys.stdout.flush()\n","\n","target = \"movie_data.csv.gz\"\n","source = \"https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch08/\" + target\n","if not os.path.isfile(target):\n","    urllib.request.urlretrieve(source, target, reporthook)\n","\n","df = pd.read_csv(target, compression='gzip')\n","\n","assert df.shape == (50000, 2)\n","df.head(3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Nl5FM9LmRnT"},"source":["# Cell 46\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","count = CountVectorizer(stop_words='english',\n","                        max_df=.1,\n","                        max_features=5000)\n","X = count.fit_transform(df['review'].values)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pMQzcs93Q9Iq"},"source":["### Expectation-maximization algorithm\n","\n","* Goal: Estimate parameters of a statistical model so you can use it to make predictions.\n","    * With a generative model, you can estimate the probability of the data given a set of parameters.\n","* Expectation -- make predictions (e.g., classify data) based on statistical model (and its presumed parameters)\n","* Maximization -- update the unknown parameters by optimizing some \"fitness\" function (e.g., prediction errors)\n","* VanderPlas demonstrates E-M with K-means [05.11-k-means](https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html)\n","    * Initialize: randomly choose K cluster centers\n","         * i.e., initialize parameters of statistical model\n","    * Step 1: Expectation -- make predictions based on statistical model\n","        * i.e., classify data by assigning samples to clusters based on distance from the K centers\n","    * Step 2: Maximization -- update the parameters based on the data and some \"fitness\" criterion\n","        * i.e., recompute the cluster centers from the data and the predicted labels from Step 1\n","    * Repeat steps 1 & 2 until done\n","        * i.e., go back to Step 1 and repeat until parameters stop changing\n","* Visualization using Old Faithful geyser data -- wait-time (delay) between eruptions vs eruption duration\n","    * Ref: [Expectation-maximization](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) -- wikipedia\n","\n","<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/69/EM_Clustering_of_Old_Faithful_data.gif\" width=\"400\"/>\n","\n"]},{"cell_type":"code","metadata":{"id":"FtJs3_oqmhNZ"},"source":["# Cell 47 (takes ~7 minutes in Colab)\n","from sklearn.decomposition import LatentDirichletAllocation\n","\n","lda = LatentDirichletAllocation(n_components=10,\n","                                random_state=123,\n","                                learning_method='batch')\n","X_topics = lda.fit_transform(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N3u_KPjkmoRo"},"source":["# Cell 48\n","lda.components_.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aTuw9XvImyTm"},"source":["# Cell 49\n","n_top_words = 5\n","feature_names = count.get_feature_names()\n","\n","for topic_idx, topic in enumerate(lda.components_):\n","    print(\"Topic %d:\" % (topic_idx + 1))\n","    print(\" \".join([feature_names[i]\n","                    for i in topic.argsort()\\\n","                        [:-n_top_words - 1:-1]]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V0hlICR3_--e"},"source":["Based on these top-ranked words for each topic, you may guess that the LDA identified the following topics:\n","1. Generally bad movies (not really a topic category)\n","2. Movies about families\n","3. War movies\n","4. Art movies\n","5. Crime movies\n","6. Horror movies\n","7. Comedy movies reviews\n","8. Movies somehow related to TV shows\n","9. Movies based on books\n","10. Action movies"]},{"cell_type":"code","metadata":{"id":"qjsclHT9GQEo"},"source":["# Cell 50\n","horror = X_topics[:, 5].argsort()[::-1]\n","\n","for iter_idx, movie_idx in enumerate(horror[:3]):\n","    print('\\nHorror movie #%d:' % (iter_idx + 1))\n","    print(df['review'][movie_idx][:300], '...')"],"execution_count":null,"outputs":[]}]}