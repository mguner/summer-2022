{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"11b-naive-bayes-20newsgroups.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMxbEXElG03DWRTA9PZOyPd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yOZfw1MC4ngN"},"source":["<table align=\"center\">\n","   <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ds5110/summer-2021/blob/master/11b-naive-bayes-20newsgroups.ipynb\">\n","<img src=\"https://github.com/ds5110/summer-2021/raw/master/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n","</table>\n"]},{"cell_type":"markdown","metadata":{"id":"Vl7_ZVTyE0uD"},"source":["# 11b -- Naive Bayes 20 newsgroups classification\n","\n","* The [multinomial distribution](https://en.wikipedia.org/wiki/Multinomial_distribution) describes the probability of observing counts among a number of categories.\n","* Multinomial naive Bayes is appropriate for features that represent counts or count rates.\n","* Similar to Gaussian naive Bayes except fitting multinomial instead of Gaussian\n","\n","### Reading\n","\n","* [05.04-Feature-Engineering.ipynb](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.04-Feature-Engineering.ipynb)\n","  * simple example of [sklearn.feature_extraction.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n","  * simple example of [sklearn.feature_extraction.TfidfdVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n","* [05.05-Naive-Bayes.ipynb](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.05-Naive-Bayes.ipynb) VanderPlas -- github\n"]},{"cell_type":"code","metadata":{"id":"BHkBdxUAIc_j"},"source":["import seaborn as sns\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i_z5USxOExFf"},"source":["from sklearn.datasets import fetch_20newsgroups\n","\n","data = fetch_20newsgroups()\n","\n","print(type(data))\n","print(dir(data))\n","\n","data.target_names"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"coIdmP-6H67a"},"source":["# Extract newsgroups for 4 categories\n","categories = ['talk.religion.misc', 'soc.religion.christian',\n","              'sci.space', 'comp.graphics']\n","train = fetch_20newsgroups(subset='train', categories=categories)\n","test = fetch_20newsgroups(subset='test', categories=categories)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9TsATY_wIEhA"},"source":["print('train.data:', type(train.data),'has length:', len(train.data))\n","print('train.target:', type(train.target),'has length:', len(train.target))\n","print('train.target_names:', train.target_names)\n","print('train.target[5]', train.target[5])\n","print('train.data[5]', train.data[5])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"16tKLqdOINL1"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(TfidfVectorizer(), MultinomialNB())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rvVb6shYIQRy"},"source":["model.fit(train.data, train.target)\n","labels = model.predict(test.data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z6j-V2mwIUiM"},"source":["from sklearn.metrics import confusion_matrix\n","mat = confusion_matrix(test.target, labels)\n","sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n","            xticklabels=train.target_names, yticklabels=train.target_names)\n","plt.xlabel('true label')\n","plt.ylabel('predicted label');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dn4xPPvhI7JY"},"source":["### Once you've trained the model, you can give it any string."]},{"cell_type":"code","metadata":{"id":"QrD4XGoRIsD3"},"source":["def predict_category(s, train=train, model=model):\n","    pred = model.predict([s])\n","    return train.target_names[pred[0]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AVErd43FIvNK"},"source":["predict_category('sending a payload to the ISS')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Jslyk9JI0VV"},"source":["predict_category('discussing islam vs atheism')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FD5fInByI5ST"},"source":["predict_category('determining the screen resolution')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l3ODTFxgJJzy"},"source":["# When to use naive Bayes (from VanderPlas)...\n","\n","* naive Bayes classifiers make such stringent assumptions about data\n","  * they will generally not perform as well as a more complicated model. \n","  * but they have several advantages:\n","    * extremely fast for both training and prediction\n","    * they provide straightforward probabilistic prediction\n","    * They are often very easily interpretable\n","    * They have very few (if any) tunable parameters\n","* These advantages make naive Bayesian classifier good as an initial baseline classification\n","  * If it performs well, then great.\n","  * If it does not perform well, then you can begin exploring more sophisticated models, with this as a baseline\n","* Naive Bayes classifiers tend to perform especially well in one of the following situations:\n","  * When the naive assumptions actually match the data (very rare in practice)\n","  * For very well-separated categories, when model complexity is less important\n","  * For very high-dimensional data, when model complexity is less important\n","The last two points seem distinct, but they actually are related: as the dimension of a dataset grows, it is much less likely for any two points to be found close together (after all, they must be close in every single dimension to be close overall). This means that clusters in high dimensions tend to be more separated, on average, than clusters in low dimensions, assuming the new dimensions actually add information. For this reason, simplistic classifiers like naive Bayes tend to work as well or better than more complicated classifiers as the dimensionality grows: once you have enough data, even a simple model can be very powerful."]}]}