{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"11c-lsa-brown.ipynb","provenance":[{"file_id":"1ruBHysovxBPTS0bwB11iBPj-VWusCoc4","timestamp":1627748105178},{"file_id":"1p950XPB6tocEs6M5cxY6WqywnDLNWgQU","timestamp":1615303037407}],"collapsed_sections":[],"authorship_tag":"ABX9TyOLWkJoTuMP/9Dz/+6T8hpu"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"f1cwXIHImD1D"},"source":["<table align=\"center\">\n","   <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ds5110/summer-2021/blob/master/11c-lsa-brown.ipynb\">\n","<img src=\"https://github.com/ds5110/summer-2021/raw/master/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n","</table>\n","\n","# 11c -- LSA Brown\n","\n","Unsupervised Latent Semantic Analysis (LSA) of the Brown corpus\n","\n","### References\n","\n","* [Brown corpus manual](http://icame.uib.no/brown/bcm.html)\n","    * [list of samples](http://icame.uib.no/brown/bcm-los.html)\n","* [NLTK](https://www.nltk.org/) -- nltk.org\n","  * [installing data](https://www.nltk.org/data.html) -- nltk.org\n","  * [corpora](https://www.nltk.org/book/ch02.html) -- nltk.org\n","  * [*Natural Language Processing with Python*](https://www.nltk.org/book/) (book updated for Python 3) -- nltk.org\n","  * [text corpora](https://www.nltk.org/book/ch02.html) (chapter 2 of NLTK book) -- nltk.org\n","* Chapter 14 of *Machine Learning Algorithms, 2nd Ed*, 2018, Giuseppe Bonaccorso, Packt\n","  * [lsa.py](https://github.com/giuseppebonaccorso/Machine-Learning-Algorithms-Second-Edition/blob/master/Chapter14/lsa.py) -- github\n","  * [lsa_1.py](https://github.com/giuseppebonaccorso/Machine-Learning-Algorithms-Second-Edition/blob/master/Chapter14/lsa_1.py) -- github\n","  * [lsa_2.py](https://github.com/giuseppebonaccorso/Machine-Learning-Algorithms-Second-Edition/blob/master/Chapter14/lsa_2.py) -- github"]},{"cell_type":"markdown","metadata":{"id":"O0x6nqzRfNDx"},"source":["# Topic modeling\n","\n","* The goal for topic modeling in NLP: analyze a corpus and identify common topics\n","* Topic: implies use of common words in a document and confirmed by multiple documents\n","* Semantic purpose is presumed to exist but is not part of the analysis\n","* Starting point is an occurrence matrix\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aXuwh6smm7i5"},"source":["# NLTK\n","\n","The Brown corpus -- the first million-word electronic corpus of English created in 1961 at Brown University."]},{"cell_type":"code","metadata":{"id":"eN3jpsTemhFr"},"source":["# Install NLTK and the Brown corpus\n","import nltk\n","from nltk.corpus import brown\n","\n","nltk.download('brown')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QG7WRRPMgRDz"},"source":["# Verify the size of the downloaded data\n","!du -h /root/nltk_data/corpora/brown/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mzgdjAV7u8H_"},"source":["# Lazy evaluation\n","\n","* Investigate the objects we downloaded from NLTK\n","* Documents are organized by genre (category)"]},{"cell_type":"code","metadata":{"id":"gdSXsBkGrSOH"},"source":["# What did we download?\n","print(type(brown))\n","sentences = brown.sents(categories=['news'])[0:500]\n","print(type(sentences))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oz37wW5Cq58Z"},"source":["!ls /root/nltk_data/*\n","!du -h /root/nltk_data/*"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zQ04LKXyQIZO"},"source":["print(brown.categories())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VAx75A3coxdp"},"source":["sentences = brown.sents(categories=['news'])[0:500]\n","assert isinstance(sentences, nltk.collections.LazySubsequence)\n","for k in range(3):\n","    print(' '.join(sentences[k]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O1ueHccei0m1"},"source":["# LSA theory\n","\n","Bonaccorso introduces the n-by-m document-term matrix $\\mathbf{M}$, where $M_{dw}$ is the term in row $d$ and column $w$. $\\mathbf{M}$ represents a corpus of $n$ documents and a vocabulary of $m$ terms. The actual value of $M_{dw}$ could be a measure (e.g., tf-idf) of the frequency with with the term $w$ appears in document $d$.\n","\n","LSA involves the SVD of $\\mathbf{M}$,\n","\n","$$\n","\\mathbf{M} = \\mathbf{USV}^T = \\mathbf{DW}\n","$$\n","\n","where the matrix $\\mathbf{D} = \\mathbf{US}$ and $\\mathbf{W} = \\mathbf{V}$. \n","\n","* Element $D_{d,k}$ of $\\mathbf{D}$ relates document $d$ to topic $k$.\n","\n","* Element $V_{k,w}$ relates topic $k$ to term $w$.\n","\n","So if we want to consider a reduced dimensional version of $\\mathbf{D}$, then we can consider only the $K$ most important eigenvalues (singular values).\n","\n","The eigenvalues (the diagonal values $S_{kk}$ of $\\mathbf{S}$) are called \"latent variables\". The term \"latent\" derives from the fact that the singular values are not actually \"observed\".\n","\n","The code in the next few sections shows results for the top $K=2$ topics. The plots show the clustering of documents according to the two 2 topics.  And the top 10 words associated with the top two topics are printed using the lookup table to associate element $V_{t,w}$ with the appropriate word."]},{"cell_type":"markdown","metadata":{"id":"AXgEdGRuyVoN"},"source":["# lsa.py\n","\n","* This is 500 documents in the category \"news\"\n","* Produces the figure on p400"]},{"cell_type":"code","metadata":{"id":"6HS_UE2xmCea"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from nltk.corpus import brown\n","\n","from scipy.linalg import svd\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","\n","# For reproducibility\n","np.random.seed(1000)\n","\n","\n","def scatter_documents(X):\n","    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n","\n","    ax.scatter(X[:, 0], X[:, 1])\n","    ax.set_xlabel('t0')\n","    ax.set_ylabel('t1')\n","    ax.grid()\n","    plt.show()\n","\n","# Compose a corpus\n","# This 500-sentence dataset reproduces the figure on p400\n","sentences = brown.sents(categories=['news'])[0:500]\n","\n","#########################################\n","# WARNING: If you uncomment this version, it takes ~20 minutes to run in Colab\n","# This uses two complete categories (total length ~8K)\n","# This will reproduce the figure on p401\n","##########################################\n","# sentences = sentences = brown.sents(categories=['news', 'fiction'])\n","\n","corpus = []\n","\n","for s in sentences:\n","    corpus.append(' '.join(s))\n","\n","# Vectorize the corpus\n","vectorizer = TfidfVectorizer(strip_accents='unicode', stop_words='english', sublinear_tf=True, use_idf=True)\n","Xc = vectorizer.fit_transform(corpus).todense()\n","\n","# Perform SVD\n","U, s, V = svd(Xc, full_matrices=False)\n","\n","# Extract a sub-space with rank=2\n","rank = 2\n","\n","Uk = U[:, 0:rank]\n","sk = np.diag(s)[0:rank, 0:rank]\n","Vk = V[0:rank, :]\n","\n","# Check the top-10 word per topic\n","Mwts = np.argsort(np.abs(Vk), axis=1)[::-1]\n","\n","for t in range(rank):\n","    print('\\nTopic ' + str(t))\n","    for i in range(10):\n","        print(vectorizer.get_feature_names()[Mwts[t, i]])\n","\n","# Compute the structure of a document\n","print('\\nSample document:')\n","print(corpus[0])\n","\n","Mdtk = Uk.dot(sk)\n","print('\\nSample document in the topic sub-space:')\n","print('d0 = %.2f*t1 + %.2f*t2' % (Mdtk[0][0], Mdtk[0][1]))\n","\n","# Show a scatter plot of all documents\n","scatter_documents(Mdtk)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K5GpzEw432k-"},"source":["# lsa_2.py\n","\n","* This uses two complete categories, news & fiction (total length ~8K)\n","* This will reproduce the figure on p401\n","* This uses TruncatedSVD for rank 2 -- It runs very fast.\n","* If you use the same corpus in the first cell, it'll take ~20 minutes in colab\n"]},{"cell_type":"code","metadata":{"id":"Hiy7juey4G0G"},"source":["from sklearn.decomposition import TruncatedSVD\n","\n","# For reproducibility\n","np.random.seed(1000)\n","\n","# Compose a corpus\n","sentences = brown.sents(categories=['news', 'fiction'])\n","corpus = []\n","\n","for s in sentences:\n","    corpus.append(' '.join(s))\n","\n","# Vectorize the corpus\n","vectorizer = TfidfVectorizer(strip_accents='unicode', stop_words='english', sublinear_tf=True, use_idf=True)\n","Xc = vectorizer.fit_transform(corpus)\n","\n","rank = 2\n","\n","# Performed a truncated SVD\n","tsvd = TruncatedSVD(n_components=rank)\n","Xt = tsvd.fit_transform(Xc)\n","\n","# Check the top-10 word per topic\n","Mwts = np.argsort(tsvd.components_, axis=1)[::-1]\n","\n","for t in range(rank):\n","    print('\\nTopic ' + str(t))\n","    for i in range(10):\n","        print(vectorizer.get_feature_names()[Mwts[t, i]])\n","\n","# Show a scatter plot of all documents\n","scatter_documents(Xt)"],"execution_count":null,"outputs":[]}]}