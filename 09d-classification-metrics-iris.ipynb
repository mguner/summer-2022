{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"09d-classification-metrics-iris.ipynb","provenance":[{"file_id":"1oOmPIbiQOao5YBApPfVMJwdJXq5Lnmf5","timestamp":1627376992885},{"file_id":"1Go1EFxF3mYyBP8E4zlx8pKX4kOg3i_As","timestamp":1626642348534},{"file_id":"1BAmLAsxsHI-GFa2sR6LepU6TCsDSLJhU","timestamp":1612289103878},{"file_id":"1dC169nQimOfKnE1HQA73g6hae-rn2Q2E","timestamp":1599600318454},{"file_id":"1kpcMOkirUx40ri9o2FUjCCt6dCnP4zet","timestamp":1599599169661}],"collapsed_sections":[],"authorship_tag":"ABX9TyN2SqRr2c+Tr9j5kX5HfGxl"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"-BOnwwBS_XCe"},"source":["<table align=\"center\">\n","   <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ds5110/summer-2021/blob/master/09d-classification-metrics-iris.ipynb\">\n","<img src=\"https://github.com/ds5110/summer-2021/raw/master/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"dT8orxhx1iCV"},"source":["\n","# 09d -- classification metrics - iris \n","\n","This builds from [09a-naive-bayes-iris.ipynb](https://github.com/ds5110/summer-2021/blob/master/09a-naive-bayes-iris.ipynb) and  [09c-logistic-regression-iris.ipynb](https://github.com/ds5110/summer-2021/blob/master/09c-logistic-regression-iris.ipynb)\n"]},{"cell_type":"markdown","metadata":{"id":"7BsATktxFdjF"},"source":["# Logit model (review)\n","\n","For linear regression\n","$$\n","y = \\beta_0 + \\beta_1 x\n","$$\n","and\n","$$\n","d_i = y_i + \\epsilon_i\n","$$\n","Warning: notation can become confusing. We're using subscripts for the regression coefficients $\\beta_i$. There are $p+1$ of these, where $p$ is the number of features in the model. We are also using subscripts for the data samples, and there are $N$ of these.\n","\n","Note that with linear regression, $y$ can take on any value from $- \\infty $ to $+ \\infty$. With logistic regression, we're modeling classes that have one of two values, yes/no or 0/1 or the equivalent. The data can take on one of these values with a certain probability that varies between 0 and 1. With logistic regression, we model the log-odds as a linear function of $y = \\beta_0 + \\beta_1 x$. \n","$$\n","\\mathrm{log} \\left( \\frac{p}{1-p} \\right) = \\mathrm{logit}(p) = y\n","$$\n","Since p can vary from 0 to 1, the log odds varies from $- \\infty $ to $+ \\infty$.\n","This is where the term logistic regression comes from. We're \"fitting\" a line to the log-odds. \n","\n","\n","We can solve this equation for $p(y)$\n","$$\n","p(y) = \\frac{1}{1 + e^{-y}} = \\mathrm{sigmoid(y)}\n","$$\n","\n","Here's the rub: $p$ is a conditional probability. Specifically, $p(1|x)$ is the probability that the true class is 1 given the observation $x$."]},{"cell_type":"code","metadata":{"id":"XiaOsZUwv2qp"},"source":["# Plotting sigmoid(y)\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","y = np.arange(-5,5,.01)\n","p = 1 / (1 + np.exp(-y))\n","\n","plt.plot(y, p)\n","plt.plot([-5, 5], [0, 0], \":k\")\n","plt.ylabel(\"p\")\n","plt.xlabel(\"y\");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rYGOG39wPQnK"},"source":["# Iris dataset\n","\n","Load the dataset from scikit learn and verify things."]},{"cell_type":"code","metadata":{"id":"xTzV9uoLdNJa"},"source":["# Quickly load and visualize the data with seaborn\n","import seaborn as sns\n","\n","df = sns.load_dataset(\"iris\")\n","\n","#sns.pairplot(df, hue=\"species\");"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8xcjmjuk4K64"},"source":["df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RuN_jPQ_J54j"},"source":["# 1-D Logistic Regression\n","\n","* simplified logistic regression using 2 classes and 1 feature\n","* fit: $y = \\beta_0 + \\beta_1 x$\n","* where: $y = \\mathrm{logit}(p) = \\mathrm{log}\\left(\\frac{p}{1-p}\\right)$\n","\n"]},{"cell_type":"code","metadata":{"id":"D-DhHwKLLk7c"},"source":["# Extract data from the dataframe (classes are strings)\n","import pandas as pd\n","\n","X = df.iloc[:, :2].values\n","Y = df['species']\n","Y = pd.factorize(Y)[0] # Convert strings to 0/1/2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ldy20rthoqmC"},"source":["# Feature scaling -- it won't affect the solution, but it makes plotting easier\n","# It's a standard preprocessing step, especially with large, multi-dimensional problems\n","from sklearn.preprocessing import StandardScaler\n","\n","sc = StandardScaler()\n","sc.fit(X)\n","X_std = sc.transform(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KqR-or1a63b7"},"source":["# Pull out 1 feature\n","feature_index = 0\n","X_1D = np.expand_dims(X_std[:, feature_index], axis=1)\n","X_1D.shape # Confirm dimensions needed for scikit-learn API"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NTwBu784TDe7"},"source":["# Unbalanced datasets\n","\n","By varying \"n\" in the next cell, you can \"unbalance\" the dataset.\n","\n","* `n = 100` -- balanced\n","* `50 < n < 100` -- unbalanced\n","* try: `n = 60` -- turns out: precision = 1, recall = 0.6"]},{"cell_type":"code","metadata":{"id":"qcOGgyv2HHqY"},"source":["# Only the first 2 classes (iris data are sorted by class, 50 samples each)\n","n = 60 # balanced with n = 100, unbalanced for 50 < n < 100\n","X_1D = X_1D[:n, :]\n","y_1D = Y[:n]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"brVo9ch3EMhp"},"source":["# 1-D logistic regression with scikit-learn\n","from sklearn.linear_model import LogisticRegression\n","\n","lr = LogisticRegression(C=1e5)\n","lr.fit(X_1D, y_1D)\n","y_pred = lr.predict(X_1D)\n","\n","# Plot data values with filled red circles\n","plt.plot(X_1D, y_1D, 'ro', label='data')\n","\n","# Extract the weights from the model\n","beta_0 = lr.intercept_\n","beta_1 = lr.coef_[0]\n","x = np.arange(-2,1,.01)\n","y = beta_0 + beta_1 * x\n","p = 1 / (1 + np.exp(-y))\n","\n","# Plot the probability of class 1\n","plt.plot(x, p, label='probability of class 1');\n","\n","# Plot the predicted values from the data\n","plt.plot(X_1D, y_pred, 'xk', label='predictions')\n","\n","# Plot the y-axis passing through the origin\n","plt.plot([0, 0], [0, 1], 'k')\n","\n","# Plot the decision boundary (dotted vertical line)\n","# Note: this corresponds to p=.5, i.e., y=0, which is x= -beta_0/beta_1\n","x_0 = - beta_0 / beta_1\n","plt.plot([x_0, x_0], [0, 1], ':k')\n","\n","# Plot the line y=.5 (dotted horizontal line)\n","plt.plot([-2, 2], [0.5, .5], ':k')\n","plt.legend()\n","plt.xlabel('y')\n","plt.ylabel('p');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"id8Lcv7FQQPx"},"source":["# Confusion matrix\n","\n","Recall: we used the confusion matrix with naive Bayes classification in [09a-naive-bayes-iris.ipynb](https://github.com/ds5110/summer-2021/blob/master/09a-naive-bayes-iris.ipynb)\n","\n","For 2 classes, a classification model has 4 possible categories of predictions:\n","\n","* TP = True Positives = # of predicted positives that are correct\n","* FP = False Positives = # of predicted positives that are wrong\n","* TN = True Negatives =  # of predicted negatives that are correct\n","* FN = False Negatives = # of predicted negatives that are wrong\n","\n","These correspond to entries in the 2-by-2 \"confusion matrix\", $C_{ij}$ as follows:\n","\n","* $ C_{ij} $ = # of observations known to be in class \"i\" (row) and predicted to be in class \"j\" (column)\n","* Unless explicitly specified with the \"labels\" keyword, classes are in sorted order.\n","* Therefore, if classes are (0,1):\n","  * $C_{0, 0}$ = TN\n","  * $C_{0,1}$ = FP\n","  * $C_{1,0}$ = FN\n","  * $C_{1,1}$ = TP\n","* You can extract these explicitly with `.ravel()`\n","  * `tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()`\n","  * Note: default unpacking with `.ravel()` is C-style (outer index cycles fastest)\n","* P = TP + FN = total # of known positives in the dataset\n","* N = TN + FP = total # of known negatives in the dataset\n","\n","### References\n","\n","* [sklearn.metrics.confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) API reference -- scikit-learn.org\n","* [sklearn.metrics.classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) -- scikit-learn.org\n","* [Confusion matrix with digits using naive Bayes](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.02-Introducing-Scikit-Learn.ipynb) (jakevdp) -- github\n","\n"]},{"cell_type":"code","metadata":{"id":"f9E1bP_zQTjq"},"source":["from sklearn.metrics import confusion_matrix\n","import matplotlib.pyplot as plt\n","\n","mat = confusion_matrix(y_1D, y_pred)\n","\n","sns.heatmap(mat, square=True, annot=True, cbar=False)\n","plt.xlabel('predicted value')\n","plt.ylabel('true value');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ugaInv4Wx-h8"},"source":["# Accuracy\n","\n","$$\n"," \\text{accuracy} \n","  = \\frac{\\text{TP + TN}}{\\text{TP + TN + FP + FN}}\n","  = \\frac{\\text{# of correct predictions}}{\\text{total # of samples in the dataset}} \n","$$\n","\n","* Important: for unbalanced datasets, \"accuracy\" may be misleading.\n","* For example: With a 100-sample dataset that has only 10 TP, a model that predicts only negative has an accuracy of 90%!\n","\n","# Other metrics\n","\n","\n","For applications in medical/diagnostic screening...\n","\n","* Sensitivity -- Answers: How well do you predict the positive class? (diabetes cases)\n","\n","$$ \n","\\text{sensitivity} \n","  = \\frac{\\text{TP}}{\\text{TP + FN}}\n","  = \\frac{\\text{# of correctly predicted positives}}{\\text{total # of positives in the dataset}}\n","$$\n","\n","* Specificity -- Answers: How well do you predict the negative class (no disease)?\n","\n","$$ \n","\\text{specificity} \n","  = \\frac{\\text{TN}}{\\text{TN + FP}}\n","  = \\frac{\\text{# of correctly predicted negatives}}{\\text{total # of negatives}}\n","$$\n","\n","For search engines, spam detection, etc...\n","\n","* Precision -- Answers the question: Of all the predicted positives, what fraction are true positive?\n","$$ \n","\\text{precision} \n","  = \\frac{\\text{TP}}{\\text{TP + FP}}\n","  = \\frac{\\text{# of true positives}}{\\text{total # of predicted positives}}\n","$$\n","\n","* Recall -- Same as Sensitivity\n","\n","True positive rate (TPR) & False positive rate (FPR)...\n","\n","* P = TP + FN = Total # of positives in the dataset\n","* N = TN + FP = Total # of negatives in the dataset\n","\n","\n","$$\n","\\text{True Positive Rate = Recall = Sensitivity} \n","  = \\frac{\\text{TP}}{\\text{P}}\n","$$\n","\n","\n","$$\n","\\text{False Positive Rate = (1 - Specificity)} \n","  = \\frac{\\text{FP}}{\\text{N}}\n","$$\n","\n","* If we optimize for Recall (Sensitivity), we want high TP and low (TP + FN). We can accomplish this while increasing FP, which will cause dismay among patients who test positive but are normal.\n","* If we optimize for Precision to keep the FP low and TP high, then this can be accomplished while making FN high (i.e., missing a diagnosis of disease, which could have dire consequences).\n","* The \"F1 score\" balances these tradeoffs\n","$$\n","\\text{F1} \n","  = 2 \\ \\frac{\\text{precision} \\times \\text{recall}}{\\text{(precision + recall)}}\n","$$\n"]},{"cell_type":"code","metadata":{"id":"Wa6_JCh7R1KT"},"source":["# EXERCISE: Investigate the way the metrics change for an unbalanced dataset \n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","print('Accuracy: %.3f' % accuracy_score(y_true=y_1D, y_pred=y_pred))\n","print('Precision: %.3f' % precision_score(y_true=y_1D, y_pred=y_pred))\n","print('Recall: %.3f' % recall_score(y_true=y_1D, y_pred=y_pred))\n","print('F1: %.3f' % f1_score(y_true=y_1D, y_pred=y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r2lAxqTxXVy1"},"source":["# ROC & AUC\n","\n","Receiver Operating Characteristic (ROC) and Area Under the Curve (AUC)\n","\n","ROC visualizes the tradeoff between sensitivity and specificity as you change the decision threshold.\n","\n","* [sklearn.metrics.roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)"]},{"cell_type":"code","metadata":{"id":"lBHohEooTmnC"},"source":["from sklearn.metrics import roc_curve, auc\n","\n","fig = plt.figure(figsize=(7, 6))\n","\n","# the .predict_proba() method returns class probabilities\n","probas = lr.fit(X_1D, y_1D).predict_proba(X_1D)\n","\n","fpr, tpr, thresholds = roc_curve(y_1D, probas[:, 1], pos_label=1)\n","roc_auc = auc(fpr, tpr)\n","plt.plot(fpr, tpr, label='ROC fold (area = {:0.2f})'.format(roc_auc))\n","\n","plt.plot([0, 1],\n","         [0, 1],\n","         linestyle='--',\n","         color=(0.6, 0.6, 0.6),\n","         label='Random guessing')\n","\n","plt.plot([0, 0, 1],\n","         [0, 1, 1],\n","         linestyle=':',\n","         color='black',\n","         label='Perfect performance')\n","\n","for i, (f, t, z) in enumerate(zip(fpr, tpr, thresholds)):\n","  plt.text(f, t, '{:.2f}'.format(z))\n","\n","plt.xlim([-0.05, 1.05])\n","plt.ylim([-0.05, 1.05])\n","plt.title('ROC with threshold values')\n","plt.xlabel('False positive rate (1-specificity)')\n","plt.ylabel('True positive rate (Sensitivity, Recall)')\n","plt.legend(loc=\"lower right\");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9eNV471tdMXb"},"source":["# Pipelines\n","\n","* Combine preprocessing and modeling in a single object\n","* We'll use a pipeline to streamline cross-validation"]},{"cell_type":"code","metadata":{"id":"uAbwK-4vdhJC"},"source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import make_pipeline\n","\n","pipe_lr = make_pipeline(StandardScaler(),\n","                        LogisticRegression(random_state=1, solver='lbfgs'))\n","\n","pipe_lr.fit(X_1D, y_1D)\n","y_pred = pipe_lr.predict(X_1D)\n","print('Test Accuracy: %.3f' % pipe_lr.score(X_1D, y_1D))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cer79a_TZJxI"},"source":["# Cross validation\n","\n","* Recall: 07-Modeling2-validation.ipynb\n","  * We looked at validation curve with simulated dataset\n","  * Model complexity & bias-variance tradeoff (polynomial least squares)\n","  * Cross-validation was built into [`sklearn.model_selection.validation_curve`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html)\n","  * We set the CV strategy with `cv=7`, which uses [`sklearn.model_selection.StratifiedKFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)\n","    * Stratified K-Folds cross-validator provides train/test indices to split data in train/test sets.\n","    * This variation of the KFold cross-validation object returns stratified folds. \n","    * The folds are made by preserving the percentage of samples for each class."]},{"cell_type":"code","metadata":{"id":"ub6Xfg_eceoh"},"source":["from sklearn.model_selection import StratifiedKFold\n","    \n","kfold = StratifiedKFold(n_splits=10).split(X_1D, y_1D)\n","\n","scores = []\n","for k, (train, test) in enumerate(kfold):\n","    pipe_lr.fit(X_1D[train], y_1D[train])\n","    score = pipe_lr.score(X_1D[test], y_1D[test])\n","    scores.append(score)\n","    print('Fold: %2d, Class dist.: %s, Acc: %.3f' % (k+1,\n","          np.bincount(y_1D[train]), score))\n","    \n","print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rCumTZy5gjCa"},"source":["# StratifiedKFold returns a \"generator\" -- inspect it with list()\n","kfold = StratifiedKFold(n_splits=10).split(X_1D, y_1D)\n","#list(kfold)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"twQpoXnye2TZ"},"source":["# This cell shows that cross validation is built into the cross_val_score object\n","from sklearn.model_selection import cross_val_score\n","\n","scores = cross_val_score(estimator=pipe_lr,\n","                         X=X_1D,\n","                         y=y_1D,\n","                         cv=10,\n","                         n_jobs=1)\n","[print('CV accuracy score: {:.3f}'.format(score)) for score in scores]\n","print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hweeKfS7lNIZ"},"source":["# ROC with Cross Validation"]},{"cell_type":"code","metadata":{"id":"Z4MKKTnFlPKG"},"source":["from numpy import interp\n","pipe_lr = make_pipeline(StandardScaler(),\n","                        LogisticRegression(penalty='l2', \n","                                           random_state=1,\n","                                           solver='lbfgs',\n","                                           C=100.0))\n","\n","cv = list(StratifiedKFold(n_splits=3).split(X_1D, y_1D))\n","\n","fig = plt.figure(figsize=(7, 6))\n","\n","mean_tpr = 0.0\n","mean_fpr = np.linspace(0, 1, 100)\n","all_tpr = []\n","\n","for i, (train, test) in enumerate(cv):\n","    probas = pipe_lr.fit(X_1D[train],\n","                         y_1D[train]).predict_proba(X_1D[test])\n","\n","    fpr, tpr, thresholds = roc_curve(y_1D[test],\n","                                     probas[:, 1],\n","                                     pos_label=1)\n","    mean_tpr += interp(mean_fpr, fpr, tpr)\n","    mean_tpr[0] = 0.0\n","    roc_auc = auc(fpr, tpr)\n","    plt.plot(fpr,\n","             tpr,\n","             label='ROC fold %d (area = %0.2f)'\n","                   % (i+1, roc_auc))\n","\n","plt.plot([0, 1],\n","         [0, 1],\n","         linestyle='--',\n","         color=(0.6, 0.6, 0.6),\n","         label='Random guessing')\n","\n","mean_tpr /= len(cv)\n","mean_tpr[-1] = 1.0\n","mean_auc = auc(mean_fpr, mean_tpr)\n","plt.plot(mean_fpr, mean_tpr, 'k--',\n","         label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n","plt.plot([0, 0, 1],\n","         [0, 1, 1],\n","         linestyle=':',\n","         color='black',\n","         label='Perfect performance')\n","\n","plt.xlim([-0.05, 1.05])\n","plt.ylim([-0.05, 1.05])\n","plt.xlabel('False positive rate')\n","plt.ylabel('True positive rate')\n","plt.legend(loc=\"lower right\");"],"execution_count":null,"outputs":[]}]}