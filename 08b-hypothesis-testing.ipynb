{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"08b-hypothesis-testing.ipynb","provenance":[{"file_id":"10oD2nkeRSCJ4MkFE-jRZ3HP0HTH9CiaV","timestamp":1626197596977}],"authorship_tag":"ABX9TyOslVGT91el+kOoiliLNFMu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"poOqENqMRtaa"},"source":["<table align=\"center\">\n","   <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ds5110/summer-2021/blob/master/08b-hypothesis-testing.ipynb\">\n","<img src=\"https://github.com/ds5110/summer-2021/raw/master/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"w3P24ICyOKeA"},"source":["# 08b Hypothesis testing\n"]},{"cell_type":"code","metadata":{"id":"RBpUKXcEbJuw"},"source":["import statsmodels.api as sm\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ROgIF-eC86it"},"source":["# Standard error for linear regression\n","\n","Recall that the standard error in using the sample mean $\\bar{\\mu}$ to estimate the mean $\\mu$ is\n","$$\n","\\mathrm{SE}(\\bar{\\mu})^2 = \\mathrm{Var}(\\bar{y}) = \\frac{\\sigma^2}{N}\n","$$\n","\n","Similar expressions can be obtained for estimating the coefficients $\\beta_0$ and $\\beta_1$ of linear regression\n","\n","$$\n","y = \\beta_0 + \\beta_1 X + \\epsilon\n","$$\n","\n","For linear regression, we can use $\\mathrm{SE}$ to compute the 95% confidence interval for the estimate $\\hat{\\beta_1}$ of $\\beta_1$:\n","\n","$$\n","\\beta_1 \\pm 2 \\mathrm{SE}(\\hat{\\beta_1})\n","$$\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2RhT8F7lsljp"},"source":["# Hypothesis test for linear regression\n","\n","Standard errors can also be used to perform hypothesis tests. The most common hypothesis test involves testing the null hypothesis:\n","\n","$$\n","H_0 : \\text{There is no relationship between X and Y}\n","$$\n","\n","versus the alternative hypothesis:\n","\n","$$\n","H_a : \\text{There is a relationship between X and Y}\n","$$\n","\n","Mathematically, these are:\n","\n","$$\n","H_0 : \\beta_1 = 0\n","$$\n","\n","versus\n","\n","$$\n","H_a : \\beta_1 \\neq 0\n","$$\n","\n","* If we accept $H_0$ then $\\beta_1 = 0$ means $y = \\beta_0 + \\epsilon$, i.e., $y$ it is not related to $x$.\n","* To test $H_0$, we need to determine whether our estimate $\\hat{\\beta}_1$ of $\\beta_1$ is sufficiently far from zero that we're confident it's not zero.\n","* Use a t-statistic for $\\hat{\\beta}_1$ and set $\\beta_1 = 0$ consistent with $H_0$:\n","$$\n","t = \\frac{\\hat{\\beta}_1 - 0}{\\mathrm{SE}(\\hat{\\beta}_1)}\n","$$\n","* Compute the probability $p$ of observing a value as large as $|t|$ or larger, \n","* This is called the \"p-value\"\n","* We reject the null hypothesis if the p-value is sufficciently small, that is, the probably of observing $t$ is too small.\n","* When the p-value is too small, we reject $H_0$ and accept $H_a$, that is, $x$ and $y$ are related because $\\beta_1 \\neq 0$. "]},{"cell_type":"markdown","metadata":{"id":"5m6CSMlfRpTS"},"source":["# Advertising dataset"]},{"cell_type":"code","metadata":{"id":"WdPC1uGwRsB0"},"source":["import pandas as pd\n","url = \"https://www.statlearning.com/s/Advertising.csv\"\n"," \n","df = pd.read_csv(url, index_col=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7NpvVeRraqaW"},"source":["# Advertising dataset: sales vs TV\n","X = df[['TV']].copy()\n","X = sm.add_constant(X)\n","y = df['sales']\n","\n","# Fit regression model\n","results = sm.OLS(y, X).fit()\n","\n","# Inspect the results\n","print(results.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dq-NndJJ6E_P"},"source":["## Hypothesis test\n","\n","* The p-value is sufficently small that we reject $H_0$ for $\\beta_0$ and $\\beta_1$\n","  * Sales and TV advertising are related to one another\n","  * $\\beta_1$ and $\\beta_0$ are not zero.\n","\n","## Confidence intervals\n","\n","* The 95% confidence intervals for $\\beta_1$ are .042 and .053\n","\n","Compare results of the next cell to p68 of ISLR."]},{"cell_type":"markdown","metadata":{"id":"ZqbjlHftCYhr"},"source":["# Advertising dataset -- the other predictors"]},{"cell_type":"code","metadata":{"id":"_daGuCWOAPm1"},"source":["# Advertising dataset: sales vs radio\n","X = df[['radio']].copy()\n","X = sm.add_constant(X)\n","y = df['sales']\n","\n","# Fit regression model\n","results = sm.OLS(y, X).fit()\n","\n","# Inspect the results\n","print(results.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8JB3T2cFAT8F"},"source":["# Advertising dataset: sales vs newspaper\n","X = df[['newspaper']].copy()\n","X = sm.add_constant(X)\n","y = df['sales']\n","\n","# Fit regression model\n","results = sm.OLS(y, X).fit()\n","\n","# Inspect the results\n","print(results.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"__jQFDad6RDc"},"source":["# Assessing model accuracy\n","\n","How accurately can we predict $y$?\n","\n","* We've rejected $H_0$ and computed confidence intervals for $\\beta_0$ and $\\beta_1$\n","* But we still don't know the true (population) values for $\\beta_0$ and $\\beta_1$\n","\n","**Residual sum of squares (RSS)**\n","\n","$$\n","\\text{RSS} = \\sum_{i=1}^N (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2\n","$$\n","\n","**Residual standard error (RSE)**\n","\n","$$\n","\\text{RSE} = \\sqrt{\\frac{\\text{RSS}}{N-2}}\n","$$\n","\n","* $ \\mathrm{RSE}$ is an estimate of variance of $\\epsilon$ for the regression model $ y = \\beta_0 + \\beta_1 X + \\epsilon $\n","* In other words, it's an estimate of the amount the response will deviate from the true regression line.\n","* These are the results for sales vs TV...\n","\n","```\n","            Intercept   TV\n","Coefficient 7.0325    0.0475\n","Std. error  0.4578    0.0027\n","t-statistic  15.36     17.67\n","p-value   < 0.0001  < 0.0001\n","```\n","\n","The next cell computes RSE. Compare to p69 of ISLR."]},{"cell_type":"code","metadata":{"id":"QNhzSRxJOaFx"},"source":["# The next 4 lines are here just to be sure we don't redefine X & y by accident\n","X = df[['TV']].copy()\n","X = sm.add_constant(X)\n","y = df['sales']\n","results = sm.OLS(y, X).fit()\n","\n","print(\"Parameters:\", results.params.to_dict())\n","print(\"Standard errors:\", results.bse.to_dict())\n","\n","# RSS\n","rss = np.square(y - results.predict()).sum()\n","\n","# RSE\n","n = df.shape[0]\n","rse = np.sqrt(rss / (n-2))\n","print(\"Residual Sum of Squares (RSS): {:.2f}\".format(rss))\n","print(\"Residual Standard Error (RSE): {:.2f}\".format(rse))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9r0tezSxb79K"},"source":["**$\\mathrm{R}^2$ statistic**\n","\n","RSE depends on the units of measurement, which makes it difficult to define what we mean by a big or small RSE. \n","\n","Introducing $\\mathrm{R}^2$:\n","\n","$$\n","\\mathrm{R}^2 = 1 - \\frac{\\mathrm{RSS}}{\\mathrm{TSS}}\n","$$\n","\n","where\n","\n","$$\n","\\mathrm{TSS} = \\sum_{i=1}^N (y_i - \\bar{y})^2\n","$$\n","\n","* $R^2$ measures the proportion of variability in $y$ that can be explained with $x$\n","* Whereas RSE depends on the units of $y$, $R^2$ is unitless and varies from 0 to 1\n","* For linear regression, $R^2$ is also the sample squared correlation between $y$ and $x$\n","\n","Q: What consitutes a \"good\" value for $R^2$?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bBzURzXSLv-6"},"source":["# Multiple linear regression\n"," \n","Q: How do we choose between TV, radio and newspaper with the advertising dataset?\n","\n","* Recall that, dollar for dollar, univariate models indicated that radio wins.\n","* Univariate models suggest that \\$1000 spent on radio would result in sales of around 200 units.\n","* In contrast, the univariate models suggest that \\$1000 spent on TV or newspaper would result in far fewer sales.\n"]},{"cell_type":"code","metadata":{"id":"HyKCRd3hMMQN"},"source":["# Python-style implementation\n","import statsmodels.api as sm\n","\n","vars = ['TV', 'radio', 'newspaper']\n","\n","# Target variable\n","y = df[\"sales\"]\n","\n","X = pd.DataFrame(1, index=df.index, columns=['Intercept']) # Intercept\n","X = X.join(df[vars])\n","X\n","\n","mod = sm.OLS(y, X)    # Describe model\n","res = mod.fit()       # Fit model\n","print(res.summary())  # Summarize model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q4erGj9VNT4y"},"source":["# Some important questions\n","\n","1. Is at least one of the predictors $x_i$ useful in predicting the response?\n","2. Do all the predictors help to explain $y$, or is only a subset of the predictors useful?\n","3. How well does the model fit the data?\n","4. Given a set of predictor values, what response value should we predict,\n","and how accurate is our prediction?"]},{"cell_type":"markdown","metadata":{"id":"3rtWACyqYkDd"},"source":["## 1. Is there at least one important predictor?\n","\n","Mathematically:\n","\n","$$\n","H_0 : \\beta_1 = \\beta_2 = \\beta_3... = \\beta_p = 0\n","$$\n","\n","versus the alternative\n","\n","$$\n","H_a : \\beta_i \\neq 0 \\text{ for a least one } i\n","$$\n","\n","This hypothethis test is performed with the F statistic\n","\n","$$\n","F = \\frac{(\\mathrm{TSS - RSS}) / p}{\\mathrm{RSS} / (p-1)}\n","$$\n","\n","where $\\mathrm{TSS} = \\sum (y_i - \\bar{y})^2$ is the \"Total Sum or Squares\".\n","\n","If the modeling assumptions are correct, along with $H_0$, then both the numerator and the denominator equal $\\sigma^2$.\n","\n","When there's no true skill, you expect $F \\approx 1$.\n","\n","On the other hand, if $H_a$ is correct, then you expect $F > 1$.\n","\n","* For the result above, $F \\gg 1$, so it's clear that there's a relationship.\n","* When N is large, $F > 1$ is sufficient\n","* When N is small, you need a larger value of $F$.\n","* Use the p-value for the F statistic, in general, to test $H_0$\n"]},{"cell_type":"markdown","metadata":{"id":"Y6QcHjqhkUmZ"},"source":["\n","## 2. What about a subset of all the predictors?\n","\n","* Each individual predictor in the result above has a p-value and t-statistic.\n","* These are equivalent to the F-test for omitting only one variable from the model\n","* In the multivariate model, there's no evidence that newspapers are important.\n","* Compare this to the conclusion you might draw from univariate regression.\n","* **BUT** be careful about avoiding an F test when $p \\gg 1$, since you expect 5% of the t-statistics to be \"significant\"\n","* The F-statistics compensates for large $p$\n","* Various statistics can be used to select the \"best\" model from a large number of candidate models, including:\n","  * Mallow's $C_p$\n","  * Akaike Information Criterion (AIC)\n","  * Bayesian Information Criterion (BIC)\n","  * Adjusted $R^2$\n","* We can't consider all possible models when $p$ is large\n","  * If $p=2$, there are $2^2 = 4$ possible models\n","  * If $p=30$, there are $2^{30}$, i.e., more than a billion\n","* The standard approaches to model selection\n","  * Forward selection\n","    * start with $H_0$\n","    * add the variable that produces the lowest RSS\n","    * continue until you reach some stopping criterion\n","  * Backward selection\n","    * start with all $p$ variables\n","    * remove the variable with the largest $p$ value\n","    * continue until you reach some stopping criterion\n","  * Mixed selection\n","    * a combination of the two other approaches\n","    * start with $H_0$ and add variables one by one\n","    * remove any variables if their p-value rises above a threshold"]},{"cell_type":"markdown","metadata":{"id":"2_sqlRNkl_OO"},"source":["## 3. How well does the model fit? \n","\n","* RSE and $R^2$ are the most common metrics for model fit\n","* $R^2$ for a multivariate model is the squared correlation between $y$ and $\\hat{y}$\n","* In fact, the least-square fit maximimizes $R^2$ among all possible linear models.\n","\n","$$\n","\\mathrm{RSE} = \\sqrt{ \\frac{1}{n-p-1} \\mathrm{RSS} }\n","$$\n","\n","Thus, whereas $R^2$ always increases as you increase $p$, RSE will decrease if the increase in $p$ is greater than the increase in RSS."]},{"cell_type":"markdown","metadata":{"id":"-BYqwR5dq-nX"},"source":["## 4. Which response value should we predict and how accurate is the prediction?\n","\n","* You can compute confidence intervals for the prediction\n","* Confidence intervals for the prediction don't consider the noise $\\epsilon$\n","* Prediction intervals include the impact of $\\epsilon$\n","* [OLSResults.get_precition() API reference docs](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLSResults.get_prediction.html) can be used to calculate both -- statmodels.org\n"]},{"cell_type":"markdown","metadata":{"id":"0oV5ae0AN4tC"},"source":["# Qualitative predictors"]},{"cell_type":"code","metadata":{"id":"7ZeblPEWN31T"},"source":[""],"execution_count":null,"outputs":[]}]}