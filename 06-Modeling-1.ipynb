{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"06-Modeling-1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOMKgDMsdW6AAptxLaCoDYo"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"sfmkoA8XXKPH"},"source":["<table align=\"center\">\n","   <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ds5110/summer-2021/blob/master/06-Modeling-1.ipynb\">\n","<img src=\"https://github.com/ds5110/summer-2021/raw/master/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"iSxrt0jKnBkD"},"source":["# 6  -- Modeling I\n","\n","Linear regression with the diamonds dataset\n","\n","* Visualizing regression models with Seaborn\n","* Linear regression with linear model\n","* Nonlinearity (linear regression with polynomials)\n","* Scaling -- price vs carats\n","* Residuals (assessing statistical assumptions)\n","* Distributions (histograms) -- signal vs noise\n","* Q-Q plots -- visualizing distributions\n","* Box plots (justifying multiple input/feature model)\n","* Categorical features\n","\n","### Data\n","\n","* [diamonds dataset](https://ggplot2.tidyverse.org/reference/diamonds.html) -- tidyverse.org\n","\n","### References\n","\n","* [scikit-learn](https://scikit-learn.org/stable/) (Machine Learning in Python) -- scikit-learn.org\n","  * [scikit-learn User Guide](https://sklearn.org/user_guide.html)\n","  * [Generalized Linear Models](https://sklearn.org/modules/linear_model.html)\n","  * [Gaussian Process Regression](https://sklearn.org/auto_examples/gaussian_process/plot_gpr_noisy_targets.html)\n","* [05.06-Linear-Regression.ipynb](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.06-Linear-Regression.ipynb) (VanderPlas) -- github\n","  * [ch10.ipynb](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch10/ch10.ipynb) (Raschka) -- github\n","* Visualization\n","    * [seaborn API](https://seaborn.pydata.org/api.html) reference docs -- pydata.org\n","        * [visualizing regression models](https://seaborn.pydata.org/tutorial/regression.html)\n","        * [seaborn scatterplot](https://seaborn.pydata.org/generated/seaborn.scatterplot.html)\n","        * [seaborn regplot](https://seaborn.pydata.org/generated/seaborn.regplot.html)\n","    * [ggplot2 geom_smooth](https://ggplot2.tidyverse.org/reference/geom_smooth.html) (R) -- tidyverse.org"]},{"cell_type":"markdown","metadata":{"id":"PcXhCVOASqzX"},"source":["<img src=\"https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch10/images/10_01.png\" width=\"500px\">\n","\n","[Figure credit](https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch10/images/10_01.png): Raschka, Sebastian, and Vahid Mirjalili. Python Machine Learning, 3rd Ed. Packt Publishing, 2019."]},{"cell_type":"markdown","metadata":{"id":"WG865snCGJon"},"source":["# Linear regression\n","\n","* Consider [seaborn](https://seaborn.pydata.org/tutorial/regression.html) for exploratory data analysis (EDA)\n","    * [Visualizing data distributions](https://seaborn.pydata.org/tutorial/distributions.html#distribution-tutorial) (tutorial) -- pydata.org\n","    * [Visualizing regression models](https://seaborn.pydata.org/tutorial/regression.html#regression-tutorial) (tutorial) -- pydata.org\n","* Use [matplotlib](https://matplotlib.org/) for basic and customizable charts and visualization\n","    * Seaborn is based on matplotlib\n","* [scikit-learn](https://scikit-learn.org/stable/modules/linear_model.html) has a variety of algorithms\n","    * \n","* [scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html) and [statsmodels](https://www.statsmodels.org/stable/index.html) have additional statistical tools\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IQ2QC243CB8R"},"source":["\n","### Reproducibility\n","\n","* [random module](https://docs.python.org/dev/library/random.html) -- python.org\n","    * [random.seed(seed)](https://docs.python.org/dev/library/random.html#random.seed)\n","    * [random.normalvariate(mu, sigma)](https://docs.python.org/dev/library/random.html#random.normalvariate)\n","* [numpy.random](https://numpy.org/doc/stable/reference/random/index.html#quick-start) -- numpy.org"]},{"cell_type":"code","metadata":{"id":"T83uu3AcmoEi"},"source":["# Generate some synthetic data: linear model plus random noise\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Model\n","w0 = 3\n","w1 = 0.042\n","xs = np.arange(0,10)\n","ys = w0 + w1 * xs\n","\n","# Noise\n","mu, sigma = 0, 0.1 # mean and standard deviation\n","random.seed(42) # for reproducibility\n","eps = [random.normalvariate(mu, sigma) for i in enumerate(xs)]\n","\n","# Data\n","data = ys + eps"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T5r0GVxuCLJw"},"source":["# Visualization with matplotlib\n","plt.plot(xs, data,'o', label='data')\n","plt.plot(xs, ys, label='model')\n","plt.ylabel('y')\n","plt.xlabel('x')\n","plt.legend();"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CbRp397f6WSm"},"source":["# Visualizing linear regression with seaborn\n","import seaborn as sns\n","import pandas as pd\n","\n","array = np.array([xs, data]).transpose()\n","df_simple = pd.DataFrame(data=array, index=xs, columns=['x', 'data'])\n","\n","sns.regplot(x='x', y='data', data=df_simple);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9VLpG1BdI2iL"},"source":["# diamonds dataset\n","\n"]},{"cell_type":"code","metadata":{"id":"8GOzN7u0I_3i"},"source":["url = \"https://github.com/tidyverse/ggplot2/raw/master/data-raw/diamonds.csv\"\n","diamonds = pd.read_csv(url)\n","\n","diamonds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FvWAnn9d15iS"},"source":["# Use opacity (alpha channel) to aid visualization with large amounts of data\n","plt.scatter(diamonds['carat'], diamonds['price'], alpha=.1);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U4OS1Ri3237m"},"source":["# Linear regression with Seaborn\n","\n","* [Seaborn regression plot](https://seaborn.pydata.org/generated/seaborn.regplot.html)\n","* Easy to investigate nonlinearity with polynomial fit\n"]},{"cell_type":"code","metadata":{"id":"yYy4C3J4KYZ1"},"source":["# Use the entire dataset (with all the data, this cell can take up to 30 seconds)\n","df = diamonds\n","\n","# Subsample the data (this runs in ~2 seconds)\n","df = df.iloc[::10, :]\n","\n","# For styling, see: https://seaborn.pydata.org/generated/seaborn.regplot.html\n","line_kws = {'color':'red'}\n","curve_kws = {'color': 'blue'}\n","\n","# Least squares line and polynomial\n","sns.scatterplot(x=\"carat\", y=\"price\", data=df, alpha=.1, color=\"black\")\n","sns.regplot(x=\"carat\", y=\"price\", data=df, order=0, scatter=False, line_kws=line_kws, label=\"line\")\n","sns.regplot(x=\"carat\", y=\"price\", data=df, order=5, scatter=False, line_kws=curve_kws, label=\"polynomial\")\n","\n","# figure size\n","fig = plt.gcf()\n","fig.set_size_inches(8, 6)\n","\n","# axis limits and legend\n","fig.gca().legend()\n","fig.gca().set_ylim([0, 20100]);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UZK26o7d-1lA"},"source":["# Linear regression with scikit-learn\n","\n","* With Scikit-Learn, algorithms are optimized for performance\n","* You can reproduce the seaborn visualizations with scikit-learn and matplotlib\n","* Scikit-learn has a consistent API designed for data science"]},{"cell_type":"code","metadata":{"id":"tjgSO5zatWYN"},"source":["# This cell reviews some data management with pandas & numpy\n","\n","# Use the entire dataset\n","df = diamonds\n","\n","# Reminder: there are two ways to extract a column from a dataframe\n","X = df['carat']\n","X2 = df[['carat']]\n","\n","# One way returns a series, the other returns a dataframe\n","assert isinstance(X, pd.core.series.Series)\n","assert isinstance(X2, pd.core.frame.DataFrame)\n","\n","# They contain the same number of variables, but they have different shapes\n","assert X.shape == (53940,)\n","assert X2.shape == (53940,1)\n","\n","# You can turn each of these into numpy arrays\n","X = X.values\n","X2 = X2.values\n","\n","assert isinstance(X, np.ndarray)\n","assert isinstance(X2, np.ndarray)\n","\n","# But they still have different shapes\n","assert X.shape == (53940,)\n","assert X2.shape == (53940,1)\n","\n","# You can use X2 as the first argument in the sklearn .fit() method\n","# If you try to use X as the first argument in .fit(), you'll get an error\n","# The .fit() method expects an 2-D matrix\n","\n","# You can turn X into a matrix as follows\n","X = X.reshape(-1,1)\n","assert X.shape == (53940,1)\n","assert X.shape == X2.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VHe22Mv4GhoN"},"source":["# Linear regression with scikit-learn\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import explained_variance_score\n","\n","# Use the entire dataset\n","df = diamonds\n","\n","# Fit the model to the data\n","model = LinearRegression()\n","model.fit(df[['carat']], df['price'])\n","\n","# Evaluate the performance\n","yhat = model.predict(df[['carat']])\n","print('Explained variance: {:.2f}%'.format(100 * explained_variance_score(df['price'], yhat)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e8WQnqdVMWcu"},"source":["# Plot the model prediction (no noise) as a smooth curve\n","X_model = [[0], [3]] # dependent variable, no noise\n","yhat_model = model.predict(X_model) # compute the model prediction\n","\n","plt.plot(X_model, yhat_model, color='k')\n","\n","# Plot the data (scatterplot)\n","plt.scatter(df['carat'], df['price'], alpha=.1)\n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nFnf-CfUSCvN"},"source":["# Evaluate model performance\n","\n","Investigate two methods of creating a train/test split.\n"]},{"cell_type":"code","metadata":{"id":"7kyeRF981uSO"},"source":["# Train-test split with scikit-learn\n","from sklearn.model_selection import train_test_split\n","\n","# Train-test split -- split the dataset\n","def my_train_test_split(X, y, test_size=.3):\n","    n_training_samples = int((1.0 - test_size) * X.shape[0])\n","\n","    X_train = X[:n_training_samples,:]\n","    y_train = y[:n_training_samples]\n","\n","    X_test = X[n_training_samples:,:]\n","    y_test = y[n_training_samples:]\n","\n","    return X_train, X_test, y_train, y_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MilAIXuz7uwF"},"source":["# Train/test split\n","\n","Choose one of two methods:\n","\n","* Method #1: simple split of the dataset with `my_train_test_split()`\n","* Method #2: `scikit-learn.model_selection.train_test_split`\n"]},{"cell_type":"code","metadata":{"id":"Ufi0oexl5AyF"},"source":["# Sample the entire dataset\n","X = df['carat'].values.reshape(-1,1)\n","y = df['price'].values\n","\n","# Method #1: Train/test split based on sequential sampling of the dataset\n","X_train, X_test, y_train, y_test = my_train_test_split(X, y, test_size=0.3)\n","\n","# Method #2: Train/test split with scikit-learn (random sampling)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x-AKE85rPVjf"},"source":["# Fit the model to the training data\n","model = LinearRegression()\n","model.fit(X_train, y_train)\n","\n","# Evaluate training performance\n","yhat_train = model.predict(X_train)\n","print('Explained variance (train): {:.2f}%'.format(100 * explained_variance_score(y_train, yhat_train)))\n","\n","# Evaluate test performance\n","yhat_test = model.predict(X_test)\n","print('Explained variance (test): {:.2f}%'.format(100 * explained_variance_score(y_test, yhat_test)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g-VP1I6fzRsx"},"source":["# Plot the model prediction (no noise) as a smooth curve\n","X_model = [[0], [3]] # dependent variable, no noise\n","yhat_model = model.predict(X_model) # compute the model prediction\n","plt.plot(X_model, yhat_model, color='k', label=\"model\")\n","\n","# Plot the data (scatterplot)\n","plt.scatter(X_train, y_train, color=['red'], alpha=.1, label=\"training data\")\n","plt.scatter(X_test, y_test, alpha=.1, label=\"testing data\")\n","plt.legend();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nM9zyJNiCglO"},"source":["\n","Data visualization explains the difference between the two methods of train/test split.\n","\n","Discussion: The results are very sensitive to the way in which the train/test split is implemented with Method #1. This might be interpreted as overfitting. However, overfitting is not what you would expect with a large dataset (more than 50K samples) and a very simple linear-regression model. But data visualization reveals that the sensitivity with Method #1 is really a symptom of model bias combined with non-uniform sampling in the dataset. In fact, with a train/test split based on randomized sampling (Method #2), training and test performance are nearly identical.\n","\n","Note: To keep things simple, the analysis below doesn't use train/test splits."]},{"cell_type":"markdown","metadata":{"id":"BFEUnS4B93Vf"},"source":["# Linear regression with polynomials in scikit-learn\n","\n","* Plots above are generated with seaborn, which is easy and good for EDA\n","* Scikit-learn is the preferred resource for production data science\n","* [polynomial regression](https://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions) -- scikit-learn"]},{"cell_type":"code","metadata":{"id":"3pWXl5Uw8HVj"},"source":["# Linear regression with polynomials is built-in with Seaborn\n","# With Scikit-Learn, you need to do it yourself.\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","# Add PolynomialFeatures of desired degree\n","# Inspect X to confirm the result (original series is in column #1\n","# Column #0 is full of ones (i.e., the intercept)\n","# To plot a line, use degree=1, in which case X_poly has 2 columns\n","poly = PolynomialFeatures(degree=5)\n","X_poly = poly.fit_transform(X)\n","\n","# Fit the model to the data\n","# Note: PolynomialFeatures adds the intercept (ones) to column 0\n","# hence fit_intercept=False, and the data gets moved to column 1\n","model = LinearRegression(fit_intercept=False)\n","model.fit(X_poly, y)\n","\n","# Plot the model prediction (no noise) as a smooth curve\n","X_model = np.arange(0, 5, .05).reshape(-1, 1) # dependent variable, no noise\n","X_model = poly.transform(X_model) # add the polynomials\n","yhat_model = model.predict(X_model) # compute the model prediction\n","plt.plot(X_model[:,1], yhat_model, color='k') # remember: column 1 is the dependent variable\n","\n","# Plot the data (scatterplot)\n","plt.scatter(X[:,0], y, alpha=.5)\n","plt.show();\n","\n","# Compute the model prediction at the data points\n","yhat = model.predict(X_poly)\n","\n","print('Explained variance: {:.2f}%'.format(100 * explained_variance_score(y, yhat)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lFqIVDS81C1w"},"source":["# Residuals\n","\n","* The simple linear regression model (above) clearly suffers from being oversimplified (high bias).\n","* Linear regression with a high-order polynomial model has problems as well.\n","    * The problems are worse where the data are sparse.\n","* Visualize the residuals to help assess model performance\n","    * In particular, check for \"structure\" in residuals that's inconsistent with random noise."]},{"cell_type":"code","metadata":{"id":"pGFNc_dRUS2v"},"source":["# Plot the residuals\n","plt.scatter(X[:,0], y - yhat, alpha=.5)\n","plt.plot([0, 5], [0,0], linestyle=\"dashed\", color=\"k\")\n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C95hqAYWWHMA"},"source":["# Exercise\n","\n","* Investigate different values for \"degree\" in PolynomialFeatures\n","    * Results:\n","    * degree = 1 is a linear fit\n","    * degree > 1 produces strange features where data are sparse\n","        * but the degree doesn't matter where data are dense\n","* Investigate the impact of sub-sampling scheme"]},{"cell_type":"code","metadata":{"id":"JoQhidZQg0mA"},"source":["# Plot the residuals for the linear model\n","df = diamonds\n","\n","sns.residplot(x=\"carat\", y=\"price\", data=df, order=5, line_kws=line_kws, label=\"linear\");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"szeZuZf1zZP7"},"source":["# Logarithmic dependency?\n","\n","* Clearly there's structure in the residuals -- is it logarithmic?\n","* `seaborn.regplot` has a `logx=True` parameter\n","    * Seaborn performs linear regression is `y ~ log(x)`\n","    * the scatterplot and regression model plots are linear in `x`"]},{"cell_type":"code","metadata":{"id":"OgbfCq2GzS99"},"source":["sns.scatterplot(x=\"carat\", y=\"price\", data=df, alpha=.1, color=\"black\")\n","sns.regplot(x=\"carat\", y=\"price\", data=df, logx=True, scatter=False, line_kws=line_kws)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_9s3D9LZ2sYS"},"source":["# Log scales\n","* We've reached a limitation with Seaborn\n","* Exponential structure suggests logarithmic scale transformation...\n","    * ...but `log(x)` isn't helpful in this case\n","* We can see that by transforming `x` ourselves with matplotlib"]},{"cell_type":"code","metadata":{"id":"3c806pTq3nqA"},"source":["# Log-x scaling for the independent variable (x = carat)\n","plt.scatter(df['carat'], df['price'], alpha=.1)\n","plt.gca().set_xscale('log');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7K7BDOmaaRKA"},"source":["* What about `log(y)`?\n","* Again, this can be visualized with matplotlib"]},{"cell_type":"code","metadata":{"id":"ItQZ7edojOpC"},"source":["# Log scaling for the dependent variable (y = price)\n","plt.scatter(df['carat'], df['price'], alpha=.1)\n","plt.gca().set_yscale('log');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ALNOfjvvaaUv"},"source":["* And log-log?"]},{"cell_type":"code","metadata":{"id":"xESdHelo4CxY"},"source":["# Data seem relatively linear with log-log scales\n","plt.scatter(df['carat'], df['price'], alpha=.1)\n","plt.gca().set_xscale('log')\n","plt.gca().set_yscale('log');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GB9NmoRd9y-6"},"source":["# Modeling logarithmic dependencies\n","\n","* log transformation of the data - create two new columns\n","* transform both feature & target variables for log-log regression\n","* perform linear regression with log-transformed variables"]},{"cell_type":"code","metadata":{"id":"hmWRQYG558Ob"},"source":["# Create a column with log(price) and a column with log(carat)\n","diamonds['log(price)'] = diamonds['price'].transform(np.log10)\n","diamonds['log(carat)'] = diamonds['carat'].transform(np.log10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CwnFoQJDkhhU"},"source":["# Linear regression with log-transformed data\n","df = diamonds\n","\n","X = df[['log(carat)']]\n","y = df['log(price)']\n","\n","# Fit the model to the data\n","model = LinearRegression()\n","model.fit(X, y)\n","\n","# Predict the data with the model\n","yhat = model.predict(X)\n","\n","# Assign model to a new column in the dataframe\n","df = df.assign(model_loglog = yhat)\n","\n","# Evaluate the performance\n","print('Explained variance: {:.2f}%'.format(100 * explained_variance_score(y, yhat)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MfHo6c8C4akg"},"source":["df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EZSO6kFis3DJ"},"source":["# Residuals with log-log scaling\n","\n","Is this better?\n","\n","Be careful about comparing explained variance -- we've rescaled the data.\n","\n","We need a way to assess the assumption of model + random noise. We'll further analyze the residuals."]},{"cell_type":"code","metadata":{"id":"F3G1GuPUnStv"},"source":["# Plot the residuals\n","X = df['log(carat)']\n","residuals = df['log(price)'] - df['model_loglog']\n","\n","plt.scatter(X, residuals, alpha=.5)\n","plt.plot([-.7, .7], [0,0], linestyle=\"dashed\", color=\"k\")\n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"29ViEpYmmU2U"},"source":["# Log-log scaling with seaborn\n","\n","A note about seaborn...\n","\n","* Seaborn is a good visualization tool for EDA, but it is not the right tool for analysis\n","    * With Seaborn, the scatter plot is drawn using PathCollection\n","    * x, y data are called \"offsets\" and **can** be recovered from the axes\n","    * the next cell gets a copy of the offsets for later use, but...\n","    * although we can do it, this approach is limited and it's error prone\n","* Scikit-learn is the right way to go"]},{"cell_type":"code","metadata":{"id":"9LtsIODo0AO1"},"source":["# Subsample the data (runs more quickly)\n","df_s = diamonds.iloc[::10, :]\n","\n","sns.scatterplot(x=\"log(carat)\", y=\"log(price)\", data=df_s, alpha=.1, color=\"black\")\n","sns.regplot(x=\"log(carat)\", y=\"log(price)\", data=df_s, order=0, scatter=False, line_kws=line_kws)\n","sns.regplot(x=\"log(carat)\", y=\"log(price)\", data=df_s, order=5, scatter=False, line_kws=curve_kws)\n","\n","# figure size\n","fig = plt.gcf()\n","fig.set_size_inches(8,6)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"csPu_s-R-Ba_"},"source":["# Plot the residuals (of the linear log-log regression model)\n","sns.residplot(x=\"log(carat)\", y=\"log(price)\", data=df_s, order=0, line_kws=line_kws, label=\"linear\")\n","\n","# Recovering the residual values from the residual plot is possible, but it's not a great idea\n","assert len(plt.gca().collections) == 1\n","offsets = plt.gca().collections[0].get_offsets()\n","\n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_OGwFdhG3eED"},"source":["# Visualizing data distributions\n","\n","* Statistical assessment of data and residuals\n","* Histograms with matplotlib\n","    * assessing the assumption of random noise\n","    * looking for [skewness](https://en.wikipedia.org/wiki/Skewness) or other departures from normal distribution\n","    * styling with the matplotlib API\n","* Clearly, diamond \"price\" is skewed"]},{"cell_type":"code","metadata":{"id":"RQqhWpjE1Xww"},"source":["from matplotlib.patches import Rectangle\n","\n","# Histogram of price is highly skewed (positive or right skew)\n","plt.hist(diamonds['price']);\n","\n","# Histogram styling. It can be done, but...\n","ax = plt.gca()\n","lines = ax.get_lines()\n","children = ax.get_children()\n","print('axes:', ax)\n","print('lines (histograms do not use lines):', lines)\n","print('children:', type(children), len(children))\n","print('children[0]:', children[0])\n","[child.set_edgecolor('k') for child in children if isinstance(child, Rectangle)];"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m_ZzO14fpAL_"},"source":["# EXERCISE\n","\n","Create a convenience function for styled histograms"]},{"cell_type":"code","metadata":{"id":"v5azNJxNgjz2"},"source":["# Convenience function for styled histograms\n","def styled_histogram(series):\n","    plt.hist(series)\n","    ax = plt.gca()\n","    children = ax.get_children()\n","    [child.set_edgecolor('k') for child in children if isinstance(child, Rectangle)];"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t8almtvnuPvF"},"source":["# EXERCISE\n","\n","* Compare distributions of price & log(price)\n","* Compare distributions residuals from log-log model"]},{"cell_type":"code","metadata":{"id":"GGFRD9sv8doz"},"source":["# Distribution of log(price) is much more symmetric\n","styled_histogram(df['log(price)'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hhqqjnnwkoXa"},"source":["# Plot the histogram of the residuals from the linear regression of log-log data\n","residuals = y - yhat\n","styled_histogram(residuals)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b95LuUpGln-Y"},"source":["# Q-Q plot\n","\n","The distributions are symmetric, but are they \"good\"? And what is a metric for \"good\"?\n","\n","* Quantiles (Q-Q plot) of data compared with theoretical probability distributions (probability plot)\n","* [Quantiles](https://en.wikipedia.org/wiki/Quantile) are values dividing a probability density into equal areas\n","* Using the scipy.stats library, default theoretical distribution is standard normal\n","* Using statsmodels, the default distribution is scipy.stats.distributions.norm\n","\n","### references\n","\n","* [scipy reference docs](https://docs.scipy.org/doc/scipy/reference/) -- scipy.org\n","* [scipy.stats.probplot()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html) -- scipy.org\n","\n"]},{"cell_type":"code","metadata":{"id":"Ftge8YF3lpxq"},"source":["from scipy import stats\n","\n","# Q-Q plot (data vs standard normal)\n","stats.probplot(offsets[:,1], plot=plt);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_QIvausDp1sH"},"source":["# EXERCISE\n","\n","Create a convenience function for a styled Q-Q plot"]},{"cell_type":"markdown","metadata":{"id":"BFcB7_AUzjly"},"source":["### styling\n","\n","`stats.probplot()` uses 2 plots types (line and scatterplot) with harsh colors\n","\n","* [Q-Q plot](https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot) -- wikipedia\n","* [scipy.stats.probplot](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html) API reference docs - scipy.org\n","    * plots quantiles of data against Normal distribution (default)\n","* Q: Can I change the colors?\n","* A: Yes, but the `scipy.stats` API docs don't speak to this.\n","    * You need to use the matplotlib API"]},{"cell_type":"code","metadata":{"id":"jKlixhd0oEqZ"},"source":["def qqplot(data):\n","    stats.probplot(data, plot=plt)\n","\n","    # Change the styling\n","    ax = plt.gca()\n","    lines = ax.get_lines() # there are two -- dots and line\n","    lines[0].set_markerfacecolor(\"steelblue\")\n","    lines[0].set_markeredgecolor(\"steelblue\")\n","    lines[0].set_alpha(0.5)\n","    lines[1].set_color('k');\n","\n","styled_histogram(residuals)\n","plt.show()\n","\n","qqplot(residuals)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SwOX91F1CBIA"},"source":["# Bad residuals -- linear scales\n","\n"]},{"cell_type":"code","metadata":{"id":"hKQUZvCEqfvW"},"source":["X_linear = df[['carat']]\n","y_linear = df['price']\n","\n","# Fit the model to the data\n","model = LinearRegression()\n","model.fit(X_linear, y_linear)\n","\n","# Evaluate the performance\n","yhat_linear = model.predict(X_linear)\n","residuals_linear = y_linear - yhat_linear\n","print('Explained variance: {:.2f}%'.format(100 * explained_variance_score(y_linear, yhat_linear)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CGd7x_qUB7G2"},"source":["styled_histogram(residuals_linear)\n","plt.show();\n","\n","qqplot(residuals_linear)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aY8lwxkqZG-o"},"source":["# Q-Q plot with statsmodels\n","import statsmodels.api as sm\n","sm.qqplot(residuals_linear, fit=True, line=\"45\")\n","\n","# Change the styling\n","ax = plt.gca()\n","lines = ax.get_lines() # there are two -- dots and line\n","lines[0].set_markerfacecolor(\"steelblue\")\n","lines[0].set_markeredgecolor(\"steelblue\")\n","lines[0].set_alpha(0.5)\n","lines[1].set_color('k');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MWG6XsRpsHY1"},"source":["# Q-Q plot with statsmodels\n","import statsmodels.api as sm\n","sm.qqplot(residuals, fit=True, line=\"45\")\n","\n","# Change the styling\n","ax = plt.gca()\n","lines = ax.get_lines() # there are two -- dots and line\n","lines[0].set_markerfacecolor(\"steelblue\")\n","lines[0].set_markeredgecolor(\"steelblue\")\n","lines[0].set_alpha(0.5)\n","lines[1].set_color('k');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uvGnjJr1_mb3"},"source":["# EXERCISE\n","\n","Visualize the relationship between residuals in log-log model and cut, clarity, color\n","* First, show students how to do a boxplot\n","* Then let them filter the residuals and visualize relationship"]},{"cell_type":"markdown","metadata":{"id":"D5GPS2avxwPX"},"source":["# Boxplots\n","\n","* [boxplot demo](https://matplotlib.org/stable/gallery/pyplots/boxplot_demo_pyplot.html)\n","* [matplotlib.pyplot.boxplot](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.boxplot.html) API reference docs"]},{"cell_type":"code","metadata":{"id":"HY4smc8wxx7_"},"source":["# Random data\n","np.random.seed(42)\n","spread = np.random.rand(50) * 100 # random numbers (mean 50) (50 of them)\n","center = np.ones(25) * 50  # constants = 50 (25 of them)\n","flier_high = np.random.rand(10) * 100 + 100 # 10 extra large values\n","flier_low = np.random.rand(10) * -100 # 10 extra small values\n","data = np.concatenate((spread, center, flier_high, flier_low))\n","assert data.shape == (95,)\n","\n","# Create the boxplot\n","fig1, ax1 = plt.subplots()\n","ax1.set_title('Basic Plot')\n","ax1.boxplot(data);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IZuBg9OC0SIZ"},"source":["# Add more random data \n","spread = np.random.rand(50) * 100\n","center = np.ones(25) * 40\n","flier_high = np.random.rand(10) * 100 + 100\n","flier_low = np.random.rand(10) * -100\n","d2 = np.concatenate((spread, center, flier_high, flier_low))\n","\n","# Create an array of dataframes, with one dataframe for each subplot\n","my_list = [data, d2, d2[::2]] # This works, but boxplot wants an array\n","# my_array = np.array(my_list)  # This conversion issues a deprecation warning\n","my_array = np.array(my_list, dtype=\"object\") # This is the right way\n","\n","fig7, ax7 = plt.subplots()\n","ax7.set_title('Multiple Samples with Different sizes')\n","ax7.boxplot(my_array); # With \"my_list\", boxplot issues a deprecation warning\n","\n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8ltEhOghLlcd"},"source":["# Other candidate predictors"]},{"cell_type":"code","metadata":{"id":"GQthitn7RjsS"},"source":["print('Unique cuts:', df['cut'].unique())\n","print('Unique colors:', df['color'].unique())\n","print('Unique clarities:', df['clarity'].unique())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UfHEYwjQ3zqg"},"source":["df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h7Ct0e-j3VSE"},"source":["# Another visualization of the same data -- Q: Is this more informative?\n","import seaborn as sns\n","\n","cuts = [\"Fair\", \"Good\", \"Very Good\", \"Premium\", \"Ideal\"]\n","\n","sns.catplot(data=df, x=\"cut\", y=\"price\", kind=\"point\", order=cuts);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GRePdnYP5zX8"},"source":["sns.catplot(data=df, x=\"cut\", y=\"log(price)\", kind=\"point\", order=cuts);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jaMjY_ef6A-q"},"source":["sns.catplot(data=df, x=\"cut\", y=\"log(price)\", kind=\"point\", order=cuts);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eAY4uqbL5fHQ"},"source":["sns.catplot(data=df, x=\"cut\", y=\"log(price)\", kind=\"box\", order=cuts);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eWMRA1ff5q9u"},"source":["df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EL3h0pZlhOSs"},"source":["## Comparing residuals of linear regression (of log-log model)\n","\n","Visualize the relationship between diamond \"cut\" and residuals"]},{"cell_type":"code","metadata":{"id":"SqBdaDi66Gix"},"source":["df['resid'] = df['log(price)'] - df['model_loglog']\n","df\n","\n","sns.catplot(data=df, x=\"cut\", y=\"resid\", kind=\"box\", order=cuts);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NR4hhNJSx82I"},"source":["# Linear regression with categorical features\n","\n","* You can do linear regression with categorical features\n","  * one-hot encoding of categorical features\n","  * use `drop=\"first\"` (to avoid colinear inputs)\n","* predict log(price)\n","* add log(carat) as a feature\n","* [6.3 Preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html) (sklearn.preprocessing package) -- scikit-learn.org\n","    * [6.3.4 Encoding categorical features](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features) -- scikit-learn.org\n","    * One-hot encoding of categorical features (indicator variables)\n","* Documentation discusses `drop=\"feature\"` parameter that avoids colinear inputs\n","    * Colinear inputs would cause non-regularized linear regression to fail"]},{"cell_type":"code","metadata":{"id":"NzyzFzE083vC"},"source":["# Multivariate linear regression of residuals from log-log model\n","# Uses one-hot encoding of the categorical features\n","from sklearn import preprocessing\n","from scipy.sparse import hstack\n","from sklearn.metrics import explained_variance_score\n","\n","enc = preprocessing.OneHotEncoder(drop=\"first\")\n","#enc = preprocessing.OneHotEncoder()\n","\n","y = df['resid']\n","X = df.loc[:, ['cut', 'color', 'clarity']].values\n","\n","X = enc.fit_transform(X)\n","\n","model = LinearRegression()\n","model.fit(X, y)\n","\n","yhat = model.predict(X)\n","\n","print('Explained variance (before): {:.2f}%'.format(100 * explained_variance_score(df['log(price)'], df['model_loglog'])))\n","print('Explained variance (after): {:.2f}%'.format(100 * explained_variance_score(df['log(price)'], df['model_loglog'] + yhat)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Moxbi0wx_jKv"},"source":["# Multivariate linear regression with one-hot encoding of categorical features\n","# This model includes log(carat) as a variable in the model\n","from sklearn import preprocessing\n","from scipy.sparse import hstack\n","from sklearn.metrics import explained_variance_score\n","\n","enc = preprocessing.OneHotEncoder(drop=\"first\")\n","#enc = preprocessing.OneHotEncoder()\n","\n","y = df['log(price)']\n","X = df.loc[:, ['cut', 'color', 'clarity']].values\n","\n","X = enc.fit_transform(X)\n","\n","print(type(X))\n","print(\"X.shape:\", X.shape)\n","print(len(enc.categories_))\n","print(enc.categories_)\n","print('df[log(carat)].shape', df['log(carat)'].values.reshape(-1,1).shape)\n","\n","X = hstack((X, df[['log(carat)']].values))\n","\n","model = LinearRegression()\n","model.fit(X, y)\n","\n","yhat = model.predict(X)\n","\n","print('Explained variance (before): {:.2f}%'.format(100 * explained_variance_score(y, df['model_loglog'])))\n","print('Explained variance (after): {:.2f}%'.format(100 * explained_variance_score(y, yhat)))"],"execution_count":null,"outputs":[]}]}