{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"09c-logistic-regression-iris.ipynb","provenance":[{"file_id":"1Go1EFxF3mYyBP8E4zlx8pKX4kOg3i_As","timestamp":1626642348534},{"file_id":"1BAmLAsxsHI-GFa2sR6LepU6TCsDSLJhU","timestamp":1612289103878},{"file_id":"1dC169nQimOfKnE1HQA73g6hae-rn2Q2E","timestamp":1599600318454},{"file_id":"1kpcMOkirUx40ri9o2FUjCCt6dCnP4zet","timestamp":1599599169661}],"collapsed_sections":[],"authorship_tag":"ABX9TyNQOSo9s+RHY+xeqrhg/BQk"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"-BOnwwBS_XCe"},"source":["<table align=\"center\">\n","   <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ds5110/summer-2021/blob/master/09c-logistic-regression-iris.ipynb\">\n","<img src=\"https://github.com/ds5110/summer-2021/raw/master/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"dT8orxhx1iCV"},"source":["\n","# 09c -- Logistic regression with the iris dataset\n","\n","* [Generalized linear model (GLM)](https://en.wikipedia.org/wiki/Generalized_linear_model) -- wikipedia\n","* [ISLR 1st Edition](https://www.statlearning.com/) -- statlearning.com\n","* [Logistic regression 3-class classifier](https://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html) (iris dataset) -- scikit-learn.org\n","* [iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) -- wikipedia"]},{"cell_type":"markdown","metadata":{"id":"GMYBubEtWFpY"},"source":["## Generalized linear model (GLM)\n","\n","* GMLs are flexible generalizations of ordinary linear regression\n","* Ordinary linear regression:\n","  * $ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\epsilon$\n","  * Expected value of the response $y$ depends linearly on the predictors, $x_p$\n","  * Errors have a normal distribution\n","* GLM:\n","  * Response relates to linear predictors with a nonlinear function\n","  * Appropriate for models that predict probability of a yes/no choice\n","* Model of binary events\n","  * Response variable has a Bernoulli distribution (response takes value 1 with probability \"p\")\n","  * Odds vs probability (we'll be modeling odds ratio)...\n","    * Consider a model that predicts whether you'll go to the beach   \n","    * Suppose the odds of going double with every 10-degree temperature rise\n","    * If it's 75 degrees and the probability of going is 0.75\n","    * Temperature rise to 85 degrees doubles the odds, not the probability\n","    * Odds go from 2:1 to 4:1, then to 8:1\n","    * Probability goes from 2/3 to 4/5 to 8 / 9\n","  * For binary events, we use the odds ratio $ \\frac{p}{1-p}$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7BsATktxFdjF"},"source":["# Logit model\n","\n","For linear regression\n","$$\n","y = \\beta_0 + \\beta_1 x\n","$$\n","and\n","$$\n","d_i = y_i + \\epsilon_i\n","$$\n","Warning: notation can become confusing. We're using subscripts for the regression coefficients $\\beta_i$. There are $p+1$ of these, where $p$ is the number of features in the model. We are also using subscripts for the data samples, and there are $N$ of these.\n","\n","Note that with linear regression, $y$ can take on any value from $- \\infty $ to $+ \\infty$. With logistic regression, we're modeling classes that have one of two values, yes/no or 0/1 or the equivalent. The data can take on one of these values with a certain probability that varies between 0 and 1. With logistic regression, we model the log-odds as a linear function of $y = \\beta_0 + \\beta_1 x$. \n","$$\n","\\mathrm{log} \\left( \\frac{p}{1-p} \\right) = \\mathrm{logit}(p) = y\n","$$\n","Since p can vary from 0 to 1, the log odds varies from $- \\infty $ to $+ \\infty$.\n","This is where the term logistic regression comes from. We're \"fitting\" a line to the log-odds. \n","\n","\n","We can solve this equation for $p(y)$\n","$$\n","p(y) = \\frac{1}{1 + e^{-y}} = \\mathrm{sigmoid(y)}\n","$$\n","\n","Here's the rub: $p$ is a conditional probability. Specifically, $p(1|x)$ is the probability that the true class is 1 given the observation $x$."]},{"cell_type":"markdown","metadata":{"id":"gzZ-QyzF-Nu8"},"source":["## Cost function\n","\n","With least-squares linear regression, we choose the regression coefficients that minimize the MSE.\n","In other words, with linear regression (ordinary least squares), the best solution is the one that minimizes the sum of squared residuals\n","\n","$$\n","J(\\beta_0, \\beta_1) = \\sum_{i=1}^N (y_i - d_i)^2\n","$$\n","Note: the subscripts on the right-hand side are the measurement index -- notation can be confusing.\n","\n","With logistic regression, we choose the $\\beta_0$ and $\\beta_i$ that maximize the likelihood that our model is the correct one given the data we observed.\n","\n","$$\n","L(\\beta_0, \\beta_1) = \\prod_{i=1}^N p_i^{d_i}(1-p_i)^{1 - d_i}\n","$$\n","\n","Minimizing the likelihood is equivalent to minimizing a cost function equal to minus the log of the likelihood\n","\n","$$\n","J(\\beta_0, \\beta_1) = \\sum_{i=1}^N \\left( d_i \\mathrm{log}(p_i) \n","+ (1-d_i)\\mathrm{log} (1-p_i) \\right)\n","$$\n","\n","Note that the contribution to the sum depends on the measured value of $d_i$, which is the label of the training data that has a value of 0 or 1. \n","* If you measure $d_i = 1$, then $\\mathrm{log}(p_i)$ contributes to $J$ and you want to make $p_i$ close to 1 so that $\\mathrm{log}(p_i)$ is close to zero. \n","* If you measure $d_i = 0$, then $\\mathrm{log}(1-p_i)$ contributes to $J$ and you maximize it by making p small.\n","\n","Although the two cost functions $J$ look very different, in fact\n","they're closely related.\n","If the errors in linear regression have a normal distribution, then the least-squares solution is also the maximum likelihood solution. You can see this by noting the form of the normal distribution. The numerator of the exponential is the square of the random error...\n","\n","$$\n","\\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{- \\frac{(x - \\mu)^2}{2\\sigma^2}}\n","$$"]},{"cell_type":"code","metadata":{"id":"XiaOsZUwv2qp"},"source":["# Plotting sigmoid(y)\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","y = np.arange(-5,5,.01)\n","p = 1 / (1 + np.exp(-y))\n","\n","plt.plot(y, p)\n","plt.plot([-5, 5], [0, 0], \":k\")\n","plt.ylabel(\"p\")\n","plt.xlabel(\"y\");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rYGOG39wPQnK"},"source":["# Iris dataset\n","\n","Load the dataset from scikit learn and verify things."]},{"cell_type":"code","metadata":{"id":"xTzV9uoLdNJa"},"source":["# Quickly load and visualize the data with seaborn\n","import seaborn as sns\n","\n","df = sns.load_dataset(\"iris\")\n","\n","sns.pairplot(df, hue=\"species\");"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8xcjmjuk4K64"},"source":["df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RuN_jPQ_J54j"},"source":["# 1-D Logistic Regression\n","\n","* simplified logistic regression using 2 classes and 1 feature\n","* fit $y = \\beta_0 + \\beta_1 x$\n","* $y = \\mathrm{logit}(p) = \\mathrm{log}\\left(\\frac{p}{1-p}\\right)$\n","\n"]},{"cell_type":"code","metadata":{"id":"D-DhHwKLLk7c"},"source":["# Extract data from the dataframe (classes are strings)\n","import pandas as pd\n","\n","X = df.iloc[:, :2].values\n","Y = df['species'].values\n","Y = pd.factorize(Y)[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ldy20rthoqmC"},"source":["# Feature scaling -- it won't affect the solution, but makes plotting easier\n","from sklearn.preprocessing import StandardScaler\n","\n","sc = StandardScaler()\n","sc.fit(X)\n","X_std = sc.transform(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KqR-or1a63b7"},"source":["# Pull out 1 feature\n","feature_index = 0\n","X_1D = np.expand_dims(X_std[:, feature_index], axis=1)\n","X_1D.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qcOGgyv2HHqY"},"source":["# Only the first 2 classes (data are sorted by class, 50 samples each)\n","X_1D = X_1D[:100, :]\n","y_1D = Y[:100]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"brVo9ch3EMhp"},"source":["# 1-D logistic regression with scikit-learn\n","from sklearn.linear_model import LogisticRegression\n","\n","lr = LogisticRegression(C=1e5)\n","lr.fit(X_1D, y_1D)\n","y_pred = lr.predict(X_1D)\n","\n","# Plot data values with filled red circles\n","plt.plot(X_1D, y_1D, 'ro', label='data')\n","\n","# Extract the weights from the model\n","beta_0 = lr.intercept_\n","beta_1 = lr.coef_[0]\n","x = np.arange(-2,1,.01)\n","y = beta_0 + beta_1 * x\n","p = 1 / (1 + np.exp(-y))\n","\n","# Plot the probability of class 1\n","plt.plot(x, p, label='probability of class 1');\n","\n","# Plot the predicted values from the data\n","plt.plot(X_1D, y_pred, 'xk', label='predictions')\n","\n","# Plot the y-axis passing through the origin\n","plt.plot([0, 0], [0, 1], 'k')\n","\n","# Plot the decision boundary (dotted vertical line)\n","# Note: this corresponds to p=.5, i.e., y=0, which is x= -beta_0/beta_1\n","x_0 = - beta_0 / beta_1\n","plt.plot([x_0, x_0], [0, 1], ':k')\n","\n","# Plot the line y=.5 (dotted horizontal line)\n","plt.plot([-2, 2], [0.5, .5], ':k')\n","plt.legend()\n","plt.xlabel('y')\n","plt.ylabel('p');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"706d5gm_KG4g"},"source":["# Multi-class Logistic Regression\n","\n","* 2 features (only 2, so we can plot the 2-D decision region)\n","* 3 classes (all 3 iris species, as integers or strings)\n","* Multi-class uses the maximum probability from OVR (One-vs-Rest)"]},{"cell_type":"code","metadata":{"id":"hz7z5xWE4PIj"},"source":["# Extract data from the dataframe (classes are strings)\n","import pandas as pd\n","\n","Y = df['species'].values\n","X = df.iloc[:, :2].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UxqBFGTQ44af"},"source":["# Load and process data from scikit-learn\n","from sklearn import datasets\n","\n","# Get the iris dataset from sklearn\n","iris = datasets.load_iris()\n","X = iris.data[:, :2]  # we only take the first two features.\n","Y = iris.target # targets are integers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uasp_oQk7oO9"},"source":["# Create an instance of Logistic Regression Classifier and fit the data.\n","logreg = LogisticRegression(C=1e5)\n","logreg.fit(X, Y)\n","\n","# Plot the decision boundary. For that, we will assign a color to each\n","# point in the mesh [x_min, x_max]x[y_min, y_max].\n","x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n","y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n","h = .02  # step size in the mesh\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n","Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n","\n","# Plotting below requires integer target variables (for colors)\n","# However, sklearn algorithm handles categorical variables automagically\n","if isinstance(Y[0], str):\n","  print(\"Converting strings to integers for plotting\")\n","  Z = pd.factorize(Z)[0]\n","  Y = pd.factorize(Y)[0]\n","\n","# Put the result into a color plot\n","Z = Z.reshape(xx.shape)\n","plt.figure(1, figsize=(4, 3))\n","plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n","\n","# Plot also the training points\n","plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n","plt.xlabel('Sepal length')\n","plt.ylabel('Sepal width')\n","\n","plt.xlim(xx.min(), xx.max())\n","plt.ylim(yy.min(), yy.max())\n","plt.xticks(())\n","plt.yticks(());"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"as4fZxrcUYS7"},"source":["# Regularization\n","\n","Regularization adds a penalty for large $\\beta_i$ (model coefficients).\n","\n","Ridge regression adds a penalty term for the size (squared magnitude -- L2 norm) of the coefficients.\n","\n","$$\n","J' = J + \\frac{1}{C} \\sum_{i=0}^p \\beta_i^2\n","$$\n","\n","LASSO adds a penalty term for the size (absolute value -- L1 norm) of the coefficients.\n","\n","$$\n","J' = J + \\alpha \\sum_{i=0}^p | \\beta_i |\n","$$\n","\n","Scikit-learn has regularization turned on by default. Statsmodels does not."]},{"cell_type":"markdown","metadata":{"id":"0VrQciTjpcRw"},"source":["# Why regularize?\n","\n","It's all about model selection, the bias-variance tradeoff, and the desire to limit model complexity.\n","\n","Review: [07-Modeling2-validation.ipynb](https://github.com/ds5110/summer-2021/blob/master/07-Modeling2-validation.ipynb)\n","\n","<img src=\"https://github.com/jakevdp/PythonDataScienceHandbook/raw/master/notebooks/figures/05.03-bias-variance.png\" >\n","\n","<img src=\"https://github.com/jakevdp/PythonDataScienceHandbook/raw/master/notebooks/figures/05.03-bias-variance-2.png\" >\n","\n","Figure credit: [05.03-Hyperparameters-and-Model-Validation.ipynb](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.03-Hyperparameters-and-Model-Validation.ipynb) VanderPlas -- github\n","\n"]},{"cell_type":"code","metadata":{"id":"6ImJekgdRyOP"},"source":["weights, params = [], []\n","for c in np.arange(-5, 5):\n","    lr = LogisticRegression(C=10.**c, random_state=1)\n","    lr.fit(X, Y)\n","    weights.append(lr.coef_[1])\n","    params.append(10.**c)\n","\n","weights = np.array(weights)\n","plt.plot(params, weights[:, 0],\n","         label='petal length')\n","plt.plot(params, weights[:, 1], linestyle='--',\n","         label='petal width')\n","plt.ylabel('weight coefficient')\n","plt.xlabel('C')\n","plt.legend()\n","plt.xscale('log')\n","plt.plot(params, 0 * np.array(params), ':k');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HFsCfmjzUFCc"},"source":["# LASSO vs Ridge Regression\n","\n","LASSO (Least Absolute Shrinkage and Selection Operator) penalizes the sum of absolute values (L1 norm) of the model coefficients, whereas ridge regression penalizes the sum of squared magnitudes (L2 norm).\n","\n","The distinction has implications for model selection. LASSO tends to successively zero-out coefficients. Ridge regression successively reduces all of them.\n","\n","<img src=\"https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch04/images/04_04.png\" width=\"400\"/>\n","\n","## L2\n","\n","<img src=\"https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch04/images/04_05.png\" width=\"400\"/>\n","\n","## L1 \n","\n","<img src=\"https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch04/images/04_06.png\" width=\"400\"/>\n","\n","Figure credits: Raschka's [ch04.ipynb](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch04/ch04.ipynb)\n"]},{"cell_type":"code","metadata":{"id":"On_u9jEpTlKi"},"source":["# L1 norm requires a special \"solver\"\n","# maximum iterations increased from default (100)\n","weights, params = [], []\n","for c in np.arange(-5, 5):\n","    lr = LogisticRegression(C=10.**c, penalty=\"l1\", \n","                            solver=\"liblinear\", max_iter=int(1e6), \n","                            random_state=1)\n","    lr.fit(X, Y)\n","    weights.append(lr.coef_[1])\n","    params.append(10.**c)\n","\n","weights = np.array(weights)\n","plt.plot(params, weights[:, 0],\n","         label='petal length')\n","plt.plot(params, weights[:, 1], linestyle='--',\n","         label='petal width')\n","plt.ylabel('weight coefficient')\n","plt.xlabel('C')\n","plt.legend()\n","plt.xscale('log')\n","plt.plot(params, 0 * np.array(params), ':k');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FJpwMn46jtlI"},"source":["# Hyperparameter tuning\n","\n","How do decide on the optimal value for \"C\"?\n","\n","Reading: [05.03-Hyperparameters-and-Model-Validation.ipynb](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.03-Hyperparameters-and-Model-Validation.ipynb)"]},{"cell_type":"code","metadata":{"id":"GgH6b86OmQp-"},"source":["# Convenience function to create a random dataset\n","import numpy as np\n","\n","def make_data(N, err=1.0, rseed=1):\n","    # randomly sample the data\n","    rng = np.random.RandomState(rseed)\n","    X = rng.rand(N, 1) ** 2\n","    y = 10 - 1. / (X.ravel() + 0.1)\n","    if err > 0:\n","        y += err * rng.randn(N)\n","    return X, y\n","\n","X, y = make_data(40)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D77I3E7gnxLD"},"source":["import matplotlib.pyplot as plt\n","import seaborn; seaborn.set()  # plot formatting\n","\n","X_test = np.linspace(-0.1, 1.1, 500)[:, None]\n","\n","plt.scatter(X.ravel(), y, color='black')\n","axis = plt.axis()\n","for degree in [1, 3, 5]:\n","    y_test = PolynomialRegression(degree).fit(X, y).predict(X_test)\n","    plt.plot(X_test.ravel(), y_test, label='degree={0}'.format(degree))\n","plt.xlim(-0.1, 1.0)\n","plt.ylim(-2, 12)\n","plt.legend(loc='best');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n5H_cjAupP54"},"source":["from sklearn.model_selection import validation_curve\n","degree = np.arange(0, 21)\n","train_score, val_score = validation_curve(PolynomialRegression(), X, y,\n","                                          'polynomialfeatures__degree', degree, cv=7)\n","\n","plt.plot(degree, np.median(train_score, 1), color='blue', label='training score')\n","plt.plot(degree, np.median(val_score, 1), color='red', label='validation score')\n","plt.legend(loc='best')\n","plt.ylim(0, 1)\n","plt.xlabel('degree')\n","plt.ylabel('score');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XXsPo5vhonLh"},"source":["Look for the \"best\" value for \"degree\" from the learning curve (degree = 3 in this case)"]},{"cell_type":"code","metadata":{"id":"amNcrrBvobFr"},"source":["# Look for the \"best\" value from the learning curve (3 in this case)\n","plt.scatter(X.ravel(), y)\n","lim = plt.axis()\n","y_test = PolynomialRegression(3).fit(X, y).predict(X_test)\n","plt.plot(X_test.ravel(), y_test);\n","plt.axis(lim);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R7fIx9OMo_jW"},"source":["Validation curve depends on dataset size"]},{"cell_type":"code","metadata":{"id":"UlX-b8H0o-mt"},"source":["# Create a larger dataset with the same properties\n","X2, y2 = make_data(200)\n","plt.scatter(X2.ravel(), y2);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WHu5cWDurJ6C"},"source":["Compare validation curves for two different dataset sizes"]},{"cell_type":"code","metadata":{"id":"SVJzhnrdqK66"},"source":["degree = np.arange(21)\n","train_score2, val_score2 = validation_curve(PolynomialRegression(), X2, y2,\n","                                            'polynomialfeatures__degree', degree, cv=7)\n","\n","plt.plot(degree, np.median(train_score2, 1), color='blue', label='training score')\n","plt.plot(degree, np.median(val_score2, 1), color='red', label='validation score')\n","plt.plot(degree, np.median(train_score, 1), color='blue', alpha=0.3, linestyle='dashed')\n","plt.plot(degree, np.median(val_score, 1), color='red', alpha=0.3, linestyle='dashed')\n","plt.legend(loc='lower center')\n","plt.ylim(0, 1)\n","plt.xlabel('degree')\n","plt.ylabel('score');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"olvvLuz9qira"},"source":["# Learning curve\n","\n","A model of given complexity will...\n","\n","* Overfit a small dataset -- training score greatly exceeds validation score  (high variance)\n","* Underfit a large dataset -- increasing dataset size will decrease training score will while increasing validation score\n","* Never have training score less than validation score -- lines will never cross\n","\n","\n","<img src=\"https://github.com/jakevdp/PythonDataScienceHandbook/raw/master/notebooks/figures/05.03-learning-curve.png\" >\n"]},{"cell_type":"code","metadata":{"id":"oBpC9JuAk5jf"},"source":["# Demonstrating the behavior for the polynomial fit on the original dataset\n","from sklearn.model_selection import learning_curve\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.linear_model import LinearRegression\n","from sklearn.pipeline import make_pipeline\n","\n","def PolynomialRegression(degree=2, **kwargs):\n","    return make_pipeline(PolynomialFeatures(degree),\n","                         LinearRegression(**kwargs))\n","\n","fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n","fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n","\n","for i, degree in enumerate([2, 9]):\n","    N, train_lc, val_lc = learning_curve(PolynomialRegression(degree),\n","                                         X, y, cv=7,\n","                                         train_sizes=np.linspace(0.3, 1, 25))\n","\n","    ax[i].plot(N, np.mean(train_lc, 1), color='blue', label='training score')\n","    ax[i].plot(N, np.mean(val_lc, 1), color='red', label='validation score')\n","    ax[i].hlines(np.mean([train_lc[-1], val_lc[-1]]), N[0], N[-1],\n","                 color='gray', linestyle='dashed')\n","\n","    ax[i].set_ylim(0, 1)\n","    ax[i].set_xlim(N[0], N[-1])\n","    ax[i].set_xlabel('training size')\n","    ax[i].set_ylabel('score')\n","    ax[i].set_title('degree = {0}'.format(degree), size=14)\n","    ax[i].legend(loc='best')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J4Asrty5tNsY"},"source":["# Hyperparameter selection in practice -- GridSearch"]},{"cell_type":"code","metadata":{"id":"zufdHmQFtYTr"},"source":["from sklearn.model_selection import GridSearchCV\n","\n","param_grid = {'polynomialfeatures__degree': np.arange(21),\n","              'linearregression__fit_intercept': [True, False],\n","              'linearregression__normalize': [True, False]}\n","\n","grid = GridSearchCV(PolynomialRegression(), param_grid, cv=7)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0WuhXGpatd9L"},"source":["grid.fit(X, y);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C85yQZiCtg9A"},"source":["grid.best_params_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P4BxpYfMtkuq"},"source":["model = grid.best_estimator_\n","\n","plt.scatter(X.ravel(), y)\n","lim = plt.axis()\n","y_test = model.fit(X, y).predict(X_test)\n","plt.plot(X_test.ravel(), y_test);\n","plt.axis(lim);"],"execution_count":null,"outputs":[]}]}