{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"09c-logistic-regression-iris.ipynb","provenance":[{"file_id":"1Go1EFxF3mYyBP8E4zlx8pKX4kOg3i_As","timestamp":1626642348534},{"file_id":"1BAmLAsxsHI-GFa2sR6LepU6TCsDSLJhU","timestamp":1612289103878},{"file_id":"1dC169nQimOfKnE1HQA73g6hae-rn2Q2E","timestamp":1599600318454},{"file_id":"1kpcMOkirUx40ri9o2FUjCCt6dCnP4zet","timestamp":1599599169661}],"collapsed_sections":[],"authorship_tag":"ABX9TyNmwBA7Wpg5MSLZvWt02IL2"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"-BOnwwBS_XCe"},"source":["<table align=\"center\">\n","   <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ds5110/summer-2021/blob/master/09c-logistic-regression-iris.ipynb\">\n","<img src=\"https://github.com/ds5110/summer-2021/raw/master/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"dT8orxhx1iCV"},"source":["\n","# 09c -- Logistic regression with the iris dataset\n","\n","* [Generalized linear model (GLM)](https://en.wikipedia.org/wiki/Generalized_linear_model) -- wikipedia\n","* [ISLR 1st Edition](https://www.statlearning.com/) -- statlearning.com\n","* [Logistic regression 3-class classifier](https://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html) (iris dataset) -- scikit-learn.org\n","* [iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) -- wikipedia"]},{"cell_type":"markdown","metadata":{"id":"GMYBubEtWFpY"},"source":["## Generalized linear model (GLM)\n","\n","* GMLs are flexible generalizations of ordinary linear regression\n","* Ordinary linear regression:\n","  * $ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\epsilon$\n","  * Expected value of the response $y$ depends linearly on the predictors\n","  * Errors have a normal distribution\n","* GLM:\n","  * Response relates to linear predictors with a nonlinear function\n","  * Appropriate for models that predict probability of a yes/no choice\n","* Model of binary events\n","  * Response variable has a Bernoulli distribution (response takes value 1 with probability \"p\")\n","  * Odds vs probability...\n","    * Consider a model that predicts whether you'll go to the beach   \n","    * Likelihood doubles with 10-degree temperature rise\n","    * If it's 75 degrees and the probability of going is 0.75\n","    * Temperature rise to 85 degrees doubles the odds, not the probability\n","    * Odds go from 2:1 to 4:1, then to 8:1\n","    * Probability goes from 2/3 to 4/5 to 8 / 9\n","  * For binary events, use the odds ratio $ \\frac{p}{1-p}$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7BsATktxFdjF"},"source":["For linear regression\n","$$\n","y = \\beta_0 + \\sum_{i=1}^p \\beta_i x_i\n","$$\n","Note that $y$ can take on any value from $- \\infty $ to $+ \\infty$.\n","With logistic regression, we model the log-odds as a linear function of $y$. \n","$$\n","\\mathrm{log} \\left( \\frac{p}{1-p} \\right) = \\mathrm{logit}(p) = y\n","$$\n","This is where the term logistic regression comes from. We're \"fitting\" a line to the log-odds. With least-squares linear regression, we minimize MSE. With logistic regression, we maximize the log likelihood. If the errors in linear regression have a normal distribution, then the least-squares solution is also the maximum likelihood solution.\n","\n","We can solve this equation for $p(y)$\n","$$\n","p(y) = \\frac{1}{1 + e^{-y}} = \\mathrm{sigmoid(y)}\n","$$\n","\n","Here's the rub: $p$ is a conditional probability. Specifically, $p(1|x)$ is the probability that the true class is 1 given the observation $x$."]},{"cell_type":"code","metadata":{"id":"XiaOsZUwv2qp"},"source":["# Plotting sigmoid(y)\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","y = np.arange(-5,5,.01)\n","p = 1 / (1 + np.exp(-y))\n","\n","plt.plot(y, p)\n","plt.plot([-5, 5], [0, 0], \":k\")\n","plt.ylabel(\"p\")\n","plt.xlabel(\"y\");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rYGOG39wPQnK"},"source":["# Iris dataset\n","\n","Load the dataset from scikit learn and verify things."]},{"cell_type":"code","metadata":{"id":"xTzV9uoLdNJa"},"source":["# Quickly load and visualize the data with seaborn\n","import seaborn as sns\n","\n","df = sns.load_dataset(\"iris\")\n","\n","sns.pairplot(df, hue=\"species\");"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8xcjmjuk4K64"},"source":["df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RuN_jPQ_J54j"},"source":["# 1-D Logistic Regression\n","\n","* simplified logistic regression using 2 classes and 1 feature\n","* fit $y = \\beta_0 + \\beta_1 x$\n","* $y = \\mathrm{logit}(p) = \\mathrm{log}\\left(\\frac{p}{1-p}\\right)$"]},{"cell_type":"code","metadata":{"id":"D-DhHwKLLk7c"},"source":["# Extract data from the dataframe (classes are strings)\n","import pandas as pd\n","\n","X = df.iloc[:, :2].values\n","Y = df['species'].values\n","Y = pd.factorize(Y)[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ldy20rthoqmC"},"source":["# Feature scaling -- it won't affect the solution, but makes plotting easier\n","from sklearn.preprocessing import StandardScaler\n","\n","sc = StandardScaler()\n","sc.fit(X)\n","X_std = sc.transform(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KqR-or1a63b7"},"source":["# Pull out 1 feature\n","feature_index = 0\n","X_1D = np.expand_dims(X_std[:, feature_index], axis=1)\n","X_1D.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qcOGgyv2HHqY"},"source":["# Only the first 2 classes (data are sorted by class, 50 samples each)\n","X_1D = X_1D[:100, :]\n","y_1D = Y[:100]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"brVo9ch3EMhp"},"source":["# 1-D logistic regression with scikit-learn\n","from sklearn.linear_model import LogisticRegression\n","\n","lr = LogisticRegression(C=1e5)\n","lr.fit(X_1D, y_1D)\n","y_pred = lr.predict(X_1D)\n","\n","# Plot data values with filled red circles\n","plt.plot(X_1D, y_1D, 'ro', label='data')\n","\n","# Extract the weights from the model\n","beta_0 = lr.intercept_\n","beta_1 = lr.coef_[0]\n","x = np.arange(-2,1,.01)\n","y = beta_0 + beta_1 * x\n","p = 1 / (1 + np.exp(-y))\n","\n","# Plot the probability of class 1\n","plt.plot(x, p, label='probability of class 1');\n","\n","# Plot the predicted values from the data\n","plt.plot(X_1D, y_pred, 'xk', label='predictions')\n","\n","# Plot the y-axis passing through the origin\n","plt.plot([0, 0], [0, 1], 'k')\n","\n","# Plot the decision boundary (dotted vertical line)\n","# Note: this corresponds to p=.5, i.e., y=0, which is x= -beta_0/beta_1\n","x_0 = - beta_0 / beta_1\n","plt.plot([x_0, x_0], [0, 1], ':k')\n","\n","# Plot the line y=.5 (dotted horizontal line)\n","plt.plot([-2, 2], [0.5, .5], ':k')\n","plt.legend()\n","plt.xlabel('y')\n","plt.ylabel('p');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"706d5gm_KG4g"},"source":["# Multi-class Logistic Regression\n","\n","* 2 features (so we can plot the 2-D decision region)\n","* 3 classes (all 3 iris species, as integers or strings)\n","* Multi-class uses the maximum probability from OVR (One-vs-Rest)"]},{"cell_type":"code","metadata":{"id":"hz7z5xWE4PIj"},"source":["# Extract data from the dataframe (classes are strings)\n","import pandas as pd\n","\n","Y = df['species'].values\n","X = df.iloc[:, :2].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UxqBFGTQ44af"},"source":["# Load and process data from scikit-learn\n","from sklearn import datasets\n","\n","# Get the iris dataset from sklearn\n","iris = datasets.load_iris()\n","X = iris.data[:, :2]  # we only take the first two features.\n","Y = iris.target # targets are integers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uasp_oQk7oO9"},"source":["# Create an instance of Logistic Regression Classifier and fit the data.\n","logreg = LogisticRegression(C=1e5)\n","logreg.fit(X, Y)\n","\n","# Plot the decision boundary. For that, we will assign a color to each\n","# point in the mesh [x_min, x_max]x[y_min, y_max].\n","x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n","y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n","h = .02  # step size in the mesh\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n","Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n","\n","# Plotting below requires integer target variables (for colors)\n","# However, sklearn algorithm handles categorical variables automagically\n","if isinstance(Y[0], str):\n","  print(\"Converting strings to integers for plotting\")\n","  Z = pd.factorize(Z)[0]\n","  Y = pd.factorize(Y)[0]\n","\n","# Put the result into a color plot\n","Z = Z.reshape(xx.shape)\n","plt.figure(1, figsize=(4, 3))\n","plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n","\n","# Plot also the training points\n","plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n","plt.xlabel('Sepal length')\n","plt.ylabel('Sepal width')\n","\n","plt.xlim(xx.min(), xx.max())\n","plt.ylim(yy.min(), yy.max())\n","plt.xticks(())\n","plt.yticks(());"],"execution_count":null,"outputs":[]}]}